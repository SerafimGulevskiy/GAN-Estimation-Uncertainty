{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00532c0-ed29-4298-9156-6441082fabd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb9c680-d105-4249-9185-5d88301400fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# print(os.getcwd())\n",
    "# os.chdir('./GAN-Estimation-Uncertainty/uncertainty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a60ee-83ed-42e1-aa9d-a4ef120ac704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0388f0-cb1c-4347-80c1-c51f055499d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bb1c6b-98ef-4b66-acb8-1705d83130f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba89acaf-9e20-4660-972f-a6f2d48f9ce3",
   "metadata": {},
   "source": [
    "### Useful sites:\n",
    "1. [Torch-Uncertainty Library](https://github.com/ensta-u2is/torch-uncertainty)\n",
    "   - A library for uncertainty estimation in deep learning, including deep learning baselines like Deep Ensembles, MC-Dropout, and more. The repository also provides tutorials.\n",
    "\n",
    "2. [Awesome Uncertainty Deep Learning Repository](https://github.com/ENSTA-U2IS/awesome-uncertainty-deeplearning)\n",
    "   - A collection of papers and resources related to Uncertainty and Deep Learning.\n",
    "\n",
    "3. [Paper on Uncertainty Estimation](https://cds.cern.ch/record/2837844/files/2210.09767.pdf)\n",
    "   - A paper from HSE describing how uncertainty estimation was applied to a GAN task in a physics experiment, specifically the LHCb experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e95992-efd2-4280-a5f3-bcc82a407640",
   "metadata": {},
   "source": [
    "### Prepare test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97685244-04f7-4e74-b23c-e05cb5238423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.sine import SinDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2352cf86-3866-4560-a607-6faf62fe53bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = SinDataset(\n",
    "                  train_data_length = 1024,\n",
    "                  train_data_length_certain = 974,\n",
    "                  multiplier_2 = 3\n",
    "    \n",
    "                )\n",
    "\n",
    "# my_dataset = SinDataset(\n",
    "#                   train_data_length = 990,\n",
    "#                   train_data_length_certain = 990\n",
    "#                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda928bb-591a-43ab-a016-bd74b7e71cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_dataset[:][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd390cb9-f7f6-4690-a2d4-5164c491af7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(my_dataset[:][1], my_dataset[:][0], 'ko', markersize=0.5)\n",
    "\n",
    "save_filename = os.path.join('datasets', 'sine')\n",
    "plt.savefig(save_filename, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5507d56b-0251-44a5-afb2-06258d1810c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    my_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723da382-9a3c-4113-898f-6ce8e43159d6",
   "metadata": {},
   "source": [
    "### GAN architecture\n",
    ">start with basic simple cGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744cef8e-69d2-42fe-8103-bdc1c93f6f65",
   "metadata": {},
   "source": [
    "Define the GAN\n",
    "\n",
    ">The GAN is designed such that a random latent number or vector is concatenated with a given variable 'x.' The GAN is then trained to learn the mapping from this combined input to the target variable 'y,' where 'y' is defined as the sine of 'x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81e9fd1-6864-47c5-9f70-cc85ba5e55c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.base_GAN import Base_Discriminator, Base_Generator, train\n",
    "from modules.optimal_batch import WeightedIntervalCrossEntropyLoss\n",
    "from modules.optimal_batch import create_batch, calculate_variance, weights_variances\n",
    "from modules.animate import plot_sine\n",
    "from modules.optimal_batch import calculate_MSE, calculate_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a247e20b-d369-4bec-a5a0-9231d253f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.optimal_batch import WeightedVarianceCrossEntropyLoss, variance4data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d5844b-5e53-4850-b78f-9b3a5fdcc088",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = WeightedVarianceCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbee4150-70ff-463c-be9d-5a17690dd64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.mnist_models import CNNClassifierWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a90ef0-2219-4188-a300-54f7912e62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modules.mnist_classifier import eval_model\n",
    "# from modules.mnist_models import CNN, CNN2, CNN3\n",
    "# CLASSIFIER = CNN3().to(device=device)\n",
    "# PATH = './mnist_tests_classifier/classifier__CNN3__1_9_32/classifier_model.pt'\n",
    "# CLASSIFIER.load_state_dict(torch.load(PATH))\n",
    "D.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0e3a76-f32c-41c1-a2d3-93aa67b4ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DWrapper:\n",
    "    def __init__(self, model, layer_index=-1, use_global_pooling=False):\n",
    "        self.model = model\n",
    "        self.activation = {}\n",
    "        self.base_transform = torchvision.transforms.Compose([\n",
    "                            torchvision.transforms.ToTensor(),\n",
    "                            torchvision.transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "                        ])\n",
    "        self.layer_index = layer_index\n",
    "        self.model.model[self.layer_index].register_forward_hook(self.get_activation('extract'))\n",
    "        self.use_global_pooling = use_global_pooling\n",
    "        \n",
    "    def get_activation(self, name):\n",
    "        def hook(model, input, output):\n",
    "            self.activation[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    def preprocess_image(self, image):\n",
    "        # Apply transformations to the image\n",
    "        preprocessed_image = self.base_transform(image)\n",
    "        # .unsqueeze(0)  # Add batch dimension\n",
    "        return preprocessed_image\n",
    "\n",
    "    def __call__(self, image, info, transform = False):\n",
    "        \n",
    "        # Preprocess the image\n",
    "        if transform:\n",
    "            preprocessed_image = self.preprocess_image(image)\n",
    "        output = self.model(image, info)\n",
    "        output = self.activation['extract']\n",
    "        \n",
    "        if self.use_global_pooling:\n",
    "            output = F.adaptive_avg_pool2d(output, (1, 1))\n",
    "\n",
    "        output = output.view(output.size(0), -1)  # Flatten to (batch_size, num_channels)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dfb7c6-5b93-46b8-bb0d-61fd5c2a415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nvariance4data(generator,\n",
    "                   discriminator_wrapper,\n",
    "                   data_loader,\n",
    "                   repeat: int = 20,\n",
    "                   device='cpu'):\n",
    "    variances = {}\n",
    "    latent_space_samples = torch.randn(repeat, 1)\n",
    "    for batch_idx, (x, info) in enumerate(data_loader):\n",
    "        batch_size = x.size(0)\n",
    "        x, info = x.to(device), info.to(device)\n",
    "        for el in info:\n",
    "            with torch.no_grad():\n",
    "                res = generator(latent_space_samples, el.repeat(repeat, 1))\n",
    "                # print()\n",
    "                res = discriminator_wrapper(res, el.repeat(repeat, 1))\n",
    "                variance = torch.var(res, dim=0)\n",
    "                variances[el.item()] = torch.sum(variance).item()\n",
    "                \n",
    "                \n",
    "    def variances2weights(variances: dict):\n",
    "        \"\"\"\n",
    "        Convert dict of variances to dict of weights,\n",
    "        so, the max weight is 1 for max value of variances and 0 if variance is 0\n",
    "        \"\"\"\n",
    "        max_variance = max(variances.values())\n",
    "        weights = {key: max(0.01, value / max_variance) for key, value in variances.items()} \n",
    "        # print(weights)\n",
    "        return weights\n",
    "\n",
    "\n",
    "    return variances2weights(variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b7854f-642a-45f6-94d9-84f138ed1f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar(nvariance4data(G, FEATURE_EXTRACTOR, train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b5602-fefd-4f9c-9520-3bae52c855a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE_EXTRACTOR = CNNClassifierWrapper(D, layer_index = -3, use_global_pooling = False)\n",
    "FEATURE_EXTRACTOR = DWrapper(D, layer_index = -5, use_global_pooling = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b9969-5b16-42f9-b884-722da03da12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_example = torch.rand(32, 1, 28, 28)  #image 28 on 28 with 1 chanel like in mnist\n",
    "for a, b in train_loader:\n",
    "    break\n",
    "    \n",
    "z = torch.randn(a.size(0), 1)\n",
    "x_fake, y_fake = G(z, b.view(-1, 1)).view(-1, 1), torch.zeros(a.size(0), 1) \n",
    "print(x_fake.size(), b.view(-1, 1).size())\n",
    "print(D(x_fake, b.view(-1, 1)).size())\n",
    "print(FEATURE_EXTRACTOR(x_fake, b.view(-1, 1)).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e50cc64-78cc-4f3b-9d4d-5309bb857731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_fake\n",
    "# y_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f8a398-bd65-4194-b5d6-aac86803066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D.eval\n",
    "# D(b)\n",
    "# D(x_fake, b.view(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c37820-8f6d-492b-ad32-98d14527b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd9dcda-915e-4e8b-95b5-76ae4314e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = variance4data(G, train_loader)\n",
    "# for batch_idx, (x, info) in enumerate(train_loader):\n",
    "#     batch_size = x.size(0)\n",
    "#     x, info = x.to(device), info.to(device)\n",
    "#     break\n",
    "    \n",
    "# G.zero_grad()\n",
    "\n",
    "# z = torch.randn(x.size(0), 1).to(device)\n",
    "# y = torch.ones(x.size(0), 1).to(device)\n",
    "# G_output = G(z, info.view(-1, 1)).view(-1, 1)\n",
    "    \n",
    "# D_output = D(G_output, info.view(-1, 1))\n",
    "\n",
    "# G_loss = loss_f(D_output, y.to(device), weights = a, conditional_info = info.view(-1, 1))\n",
    "# G_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0027054-520c-42bb-b69b-53b0a7df7eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e656c-40fb-4f85-a989-97218b6d09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2010aa4d-5c9f-46ba-a3ce-5fc060655984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model parametrs\n",
    "# learning_rate_G = 0.0001\n",
    "# learning_rate_D = 0.0004\n",
    "# lr = 0.001\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 1000\n",
    "NUM_BATCHES = math.ceil(len(train_loader.dataset)/train_loader.batch_size)\n",
    "print(f'NUM_BATCHES: {NUM_BATCHES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c74d7-c9e0-43f9-b576-d541d9c3a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BATCHES = math.ceil(len(train_loader.dataset)/train_loader.batch_size)\n",
    "print(f'NUM_BATCHES: {NUM_BATCHES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267339c0-317d-41ce-a18a-f85331c0a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_function = nn.BCELoss()\n",
    "loss_function = WeightedIntervalCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3120e7d6-1327-42e6-a6d1-1e027226e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# D_optimizer = optim.Adam(D.parameters(), lr=learning_rate_G, betas=(0.5, 0.999))\n",
    "# G_optimizer = optim.Adam(G.parameters(), lr=learning_rate_D, betas=(0.5, 0.999))\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "max_lr = 0.0005\n",
    "\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 8001)\n",
    "# scheduler_D = CosineAnnealingWarmRestarts(D_optimizer, T_0 = 16001)\n",
    "# scheduler_G = CosineAnnealingWarmRestarts(G_optimizer, T_0 = 16001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6b727c-ece5-4dfe-ac5c-08b7cda6e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "EPOCHS = 1000\n",
    "BATCHES = 32\n",
    "steps = []\n",
    "lrs = []\n",
    "model = Base_Discriminator().to(device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 960)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 16001)\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in range(BATCHES):\n",
    "        scheduler.step()\n",
    "        lrs.append(scheduler.get_last_lr()[0])\n",
    "        steps.append(epoch * BATCHES + batch)\n",
    "\n",
    "plt.figure()\n",
    "plt.legend()\n",
    "plt.plot(steps, lrs, label='OneCycle')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7d70bc-959f-4dc2-955e-e880ecac7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCHES = 10\n",
    "steps = []\n",
    "lrs = []\n",
    "model = Base_Discriminator().to(device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=10,total_steps=EPOCHS * BATCHES)\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in range(BATCHES):\n",
    "        scheduler.step()\n",
    "        lrs.append(scheduler.get_last_lr()[0])\n",
    "        steps.append(epoch * BATCHES + batch)\n",
    "\n",
    "plt.figure()\n",
    "plt.legend()\n",
    "plt.plot(steps, lrs, label='OneCycle')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec06f2-feab-444e-ab27-829593a3d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "#                                                num_warmup_steps=num_warmup_steps,\n",
    "#                                                num_training_steps=len(train_dataloader) * num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27ae573-44a9-4c14-9f1b-27bfd0b51b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8639302-af79-4268-90b4-75e1140c93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(G, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8af584-2cda-4882-8f15-e6bd2459e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G.eval\n",
    "torch.tensor(torch.linspace(0, math.pi, 10)[3].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1f4e5-11da-4a00-9701-3169b7da523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cdccd2-1e1c-4551-9342-76da01845c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_MSE(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf1d33-cd98-424a-a7a4-d5ac550f8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_MSE(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba9e94d-a6b5-4620-92d0-3cc028592c69",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0341961f-fb2a-4e31-a168-855482b475b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME = 'base_gan__variance__2_5'\n",
    "# NAME = 'base_gan__1_1'\n",
    "# NAME = 'base_gan__1_1'\n",
    "# NAME = 'base_gan__variance_10_split__any_wieights__2_1'\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__3_1'\n",
    "# NAME = 'base_gan__sin_confidence_all__variance_10_split__accurate_wieights__8_1'\n",
    "# NAME = 'base_gan__sin_confidence_all_1,5_pi__variance_10_split__accurate_wieights__9_1'\n",
    "# NAME = 'base_gan__lr_0,0001__10_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights_0,01__lr_0,0001__12_1'\n",
    "# NAME = 'base_gan__part_dataset__lr_0,0001__9_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights_0,01_OneCycleLR__lr_0,0005__1_1'\n",
    "\n",
    "save_path = '/Users/serafim/Desktop/Job/projects/science/hse/GAN-Estimation-Uncertainty/uncertainty/tests/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9007b698-d693-44c8-88fe-32eaee07a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "-np.log(1)\n",
    "# # %matplotlib inline\n",
    "# %config InlineBackend.close_figures = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcade96a-d17f-4b8d-866c-15a0ebd785de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.var()\n",
    "a = np.array([[1, 1, 1, 2, 2, 2]])\n",
    "np.var(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5a3f37-a610-47ca-bd97-fd8f2791b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_EPOCHS\n",
    "from modules.base_GAN import DWrapper, nvariance4data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0393eb1f-41f0-4c4d-93da-8081719fa4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_EXTRACTOR = DWrapper(G, layer_index=-2, use_global_pooling=False)\n",
    "FEATURE_EXTRACTOR.register_hook()  # Register the hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be305c-f00e-49eb-9811-192284d61d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21834946-623b-44a0-a788-a8f8102e37a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in train_loader:\n",
    "    break\n",
    "z = torch.randn(a.size(0), 1).to(device)\n",
    "print(z.size())\n",
    "# y = torch.ones(x.size(0), 1).to(device)\n",
    "#     # print(z.size(), y.size())\n",
    "    \n",
    "#     # fake_info = 2 * math.pi * torch.rand(x.size(0)).view(-1, 1).to(device)#fake info maybe is crooectly to take from data or from the same distribution, with the same density\n",
    "# G_output = G(z, info.view(-1, 1)).view(-1, space_dimension)\n",
    "\n",
    "    # weights = nvariance4data(G, FEATURE_EXTRACTOR, data_loader)\n",
    "    # FEATURE_EXTRACTOR.remove_hook()  \n",
    "FEATURE_EXTRACTOR(z, b.view(-1, 1)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b1b5cd-77e5-4603-a84f-b2f060629630",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e9301-6e23-40f5-ade5-8c7d5e29f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# ts = []\n",
    "\n",
    "# for i in tqdm(range(1000)):\n",
    "#     t1 = time.time()\n",
    "    \n",
    "#     # FEATURE_EXTRACTOR.to(device)\n",
    "#     FEATURE_EXTRACTOR = DWrapper(D, layer_index = -5, use_global_pooling = False)\n",
    "#     weights = nvariance4data(G, FEATURE_EXTRACTOR, train_loader, device = 'cpu')\n",
    "#     torch.cuda.empty_cache()\n",
    "#     ts.append(time.time() - t1)\n",
    "#     FEATURE_EXTRACTOR.activation = {}\n",
    "#     # print(time.time() - t1)\n",
    "    \n",
    "ts = []\n",
    "for i in tqdm(range(300)):\n",
    "    t1 = time.time()\n",
    "    FEATURE_EXTRACTOR = DWrapper(D, layer_index=-5, use_global_pooling=False)\n",
    "    FEATURE_EXTRACTOR.register_hook()  # Register the hook\n",
    "    weights = nvariance4data(G, FEATURE_EXTRACTOR, train_loader)\n",
    "    FEATURE_EXTRACTOR.remove_hook()  # Remove the hook when done\n",
    "    ts.append(time.time() - t1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005f5f3-18b4-40a8-b625-d3443a3659e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea0e7a-4a06-4c19-b350-7de84d4125d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar(ts)\n",
    "\n",
    "plt.plot(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f37c95-576f-47ef-bcc5-d18064258854",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "\n",
    "loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "\n",
    "NAME = f'base_gan__variance__10_split__accurate_wieights_0,01_OneCycleLR__lr_0,0001__2'\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.0005 \n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e276a1-aa44-4418-ae66-3138c6e1b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "\n",
    "loss_function = WeightedVarianceCrossEntropyLoss()\n",
    "# loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "\n",
    "NAME = f'base_gan__variance__each_point__accurate_wieights_0,01_OneCycleLR__lr_0,0001__6'\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.0005 \n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291e3864-5cf2-45ca-a8c3-1613c473dad3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D_losses_final, G_losses_final, Variances, Weights_variance, s = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = True,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    n_split = 10,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G,\n",
    "    info_n = 200,\n",
    ")\n",
    "# plot_sine(G, save_path = save_path, name = NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0debdddc-7e26-4a6c-a3bb-f465e93d02f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "D_losses_final, G_losses_final, Variances, Weights_variance, s = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = True,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    n_split = 10,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G,\n",
    "    info_n = 200,\n",
    ")\n",
    "# plot_sine(G, save_path = save_path, name = NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a6dd0d-4a15-483f-a1ca-1ed999cc7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to plot the metrics\n",
    "def plot_metrics(metrics: List[Tuple[float, float, float, float, float, float]], title: str):\n",
    "    epochs = range(1, len(metrics) + 1)\n",
    "    \n",
    "    mse1_values = [m[0] for m in metrics]\n",
    "    mse2_values = [m[1] for m in metrics]\n",
    "    var1_values = [m[2] for m in metrics]\n",
    "    var2_values = [m[3] for m in metrics]\n",
    "    entropy1_values = [m[4] for m in metrics]\n",
    "    entropy2_values = [m[5] for m in metrics]\n",
    "    \n",
    "    # Plot the metrics\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(3, 2, 1)\n",
    "    # plt.gca().set_ylim([0, 0.1])\n",
    "    plt.plot(epochs, mse1_values, 'b', label='MSE1')\n",
    "    plt.title('MSE1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 2)\n",
    "    plt.plot(epochs, mse2_values, 'r', label='MSE2')\n",
    "    plt.title('MSE2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(3, 2, 3)\n",
    "    # plt.gca().set_ylim([0, 0.1])\n",
    "    plt.plot(epochs, var1_values, 'g', label='Var1')\n",
    "    plt.title('Var1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 4)\n",
    "    plt.plot(epochs, var2_values, 'c', label='Var2')\n",
    "    plt.title('Var2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.plot(epochs, entropy1_values, 'm', label='Entropy1')\n",
    "    plt.title('Entropy1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 6)\n",
    "    plt.plot(epochs, entropy2_values, 'y', label='Entropy2')\n",
    "    plt.title('Entropy2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_metrics(s, 'Metrics Over Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d855f6a-6b50-4356-9fb8-1c90e61fcb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to plot the metrics\n",
    "def plot_metrics(metrics: List[Tuple[float, float, float, float, float, float]], title: str):\n",
    "    epochs = range(1, len(metrics) + 1)\n",
    "    \n",
    "    mse1_values = [m[0] for m in metrics]\n",
    "    mse2_values = [m[1] for m in metrics]\n",
    "    var1_values = [m[2] for m in metrics]\n",
    "    var2_values = [m[3] for m in metrics]\n",
    "    entropy1_values = [m[4] for m in metrics]\n",
    "    entropy2_values = [m[5] for m in metrics]\n",
    "    \n",
    "    # Plot the metrics\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(3, 2, 1)\n",
    "    # plt.gca().set_ylim([0, 0.1])\n",
    "    plt.plot(epochs, mse1_values, 'b', label='MSE1')\n",
    "    plt.title('MSE1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 2)\n",
    "    plt.plot(epochs, mse2_values, 'r', label='MSE2')\n",
    "    plt.title('MSE2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(3, 2, 3)\n",
    "    # plt.gca().set_ylim([0, 0.1])\n",
    "    plt.plot(epochs, var1_values, 'g', label='Var1')\n",
    "    plt.title('Var1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 4)\n",
    "    plt.plot(epochs, var2_values, 'c', label='Var2')\n",
    "    plt.title('Var2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.plot(epochs, entropy1_values, 'm', label='Entropy1')\n",
    "    plt.title('Entropy1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 6)\n",
    "    plt.plot(epochs, entropy2_values, 'y', label='Entropy2')\n",
    "    plt.title('Entropy2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_metrics(s, 'Metrics Over Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c404c-f1da-4bf3-9e26-dd49490c1889",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/serafim/Desktop/Job/projects/science/hse/GAN-Estimation-Uncertainty/uncertainty/tests/base_gan__variance__each_point__accurate_wieights_0,01_OneCycleLR__lr_0,0001__2/metrics.pkl', 'wb') as f:\n",
    "    pickle.dump(s, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8d8aa4-4cd4-470b-b198-2e7a92bbaee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf153d-be88-4c56-85e8-44edf406e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "loss_function = nn.BCELoss()\n",
    "max_lr = 0.0005\n",
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "NAME = f'base_gan__lr_0,001_OneCycleLR__7'\n",
    "\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance, ss = train(\n",
    "        num_epochs = NUM_EPOCHS,\n",
    "        data_loader = train_loader,\n",
    "        D = D,\n",
    "        G = G,\n",
    "        D_optimizer = D_optimizer,\n",
    "        G_optimizer = G_optimizer, \n",
    "        criterion = loss_function,\n",
    "        device = device,\n",
    "        plot_process = True,\n",
    "        save_path = save_path,\n",
    "        name = f'{NAME}',\n",
    "        weights_interval = False,\n",
    "        # plot_info = True,\n",
    "        animate_bar_var = True,\n",
    "        progress_generator = True,\n",
    "        scheduler_D = scheduler_D,\n",
    "        scheduler_G = scheduler_G,\n",
    "        info_n = NUM_EPOCHS,\n",
    "        # save_mse = 'G_mse.pkl'\n",
    "        # n_split = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc87f17-6de7-45c1-98d8-2325e7cd5498",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "loss_function = nn.BCELoss()\n",
    "max_lr = 0.0005\n",
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "NAME = f'base_gan__lr_0,001_OneCycleLR__8'\n",
    "\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance, ss = train(\n",
    "        num_epochs = NUM_EPOCHS,\n",
    "        data_loader = train_loader,\n",
    "        D = D,\n",
    "        G = G,\n",
    "        D_optimizer = D_optimizer,\n",
    "        G_optimizer = G_optimizer, \n",
    "        criterion = loss_function,\n",
    "        device = device,\n",
    "        plot_process = True,\n",
    "        save_path = save_path,\n",
    "        name = f'{NAME}',\n",
    "        weights_interval = False,\n",
    "        # plot_info = True,\n",
    "        animate_bar_var = True,\n",
    "        progress_generator = True,\n",
    "        scheduler_D = scheduler_D,\n",
    "        scheduler_G = scheduler_G,\n",
    "        info_n = NUM_EPOCHS,\n",
    "        # save_mse = 'G_mse.pkl'\n",
    "        # n_split = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed0913a-3a5d-48c7-bbc1-00ba242ea870",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "loss_function = nn.BCELoss()\n",
    "max_lr = 0.0005\n",
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "NAME = f'base_gan__lr_0,001_OneCycleLR__9'\n",
    "\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance, ss = train(\n",
    "        num_epochs = NUM_EPOCHS,\n",
    "        data_loader = train_loader,\n",
    "        D = D,\n",
    "        G = G,\n",
    "        D_optimizer = D_optimizer,\n",
    "        G_optimizer = G_optimizer, \n",
    "        criterion = loss_function,\n",
    "        device = device,\n",
    "        plot_process = True,\n",
    "        save_path = save_path,\n",
    "        name = f'{NAME}',\n",
    "        weights_interval = False,\n",
    "        # plot_info = True,\n",
    "        animate_bar_var = True,\n",
    "        progress_generator = True,\n",
    "        scheduler_D = scheduler_D,\n",
    "        scheduler_G = scheduler_G,\n",
    "        info_n = NUM_EPOCHS,\n",
    "        # save_mse = 'G_mse.pkl'\n",
    "        # n_split = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22872cd9-d852-400e-9724-7500e13a6534",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "loss_function = nn.BCELoss()\n",
    "max_lr = 0.0005\n",
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "NAME = f'base_gan__lr_0,001_OneCycleLR__10'\n",
    "\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance, ss = train(\n",
    "        num_epochs = NUM_EPOCHS,\n",
    "        data_loader = train_loader,\n",
    "        D = D,\n",
    "        G = G,\n",
    "        D_optimizer = D_optimizer,\n",
    "        G_optimizer = G_optimizer, \n",
    "        criterion = loss_function,\n",
    "        device = device,\n",
    "        plot_process = True,\n",
    "        save_path = save_path,\n",
    "        name = f'{NAME}',\n",
    "        weights_interval = False,\n",
    "        # plot_info = True,\n",
    "        animate_bar_var = True,\n",
    "        progress_generator = True,\n",
    "        scheduler_D = scheduler_D,\n",
    "        scheduler_G = scheduler_G,\n",
    "        info_n = NUM_EPOCHS,\n",
    "        # save_mse = 'G_mse.pkl'\n",
    "        # n_split = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f86e6-fd0f-4eb1-9840-daf6c82c9b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d3b9c7-07a1-4d60-9589-e9cf025fecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to plot the metrics\n",
    "def plot_metrics(metrics: List[Tuple[float, float, float, float, float, float]], title: str):\n",
    "    epochs = range(1, len(metrics) + 1)\n",
    "    \n",
    "    mse1_values = [m[0] for m in metrics]\n",
    "    mse2_values = [m[1] for m in metrics]\n",
    "    var1_values = [m[2] for m in metrics]\n",
    "    var2_values = [m[3] for m in metrics]\n",
    "    entropy1_values = [m[4] for m in metrics]\n",
    "    entropy2_values = [m[5] for m in metrics]\n",
    "    \n",
    "    # Plot the metrics\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(3, 2, 1)\n",
    "    # plt.gca().set_ylim([0, 0.1])\n",
    "    plt.plot(epochs, mse1_values, 'b', label='MSE1')\n",
    "    plt.title('MSE1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 2)\n",
    "    plt.plot(epochs, mse2_values, 'r', label='MSE2')\n",
    "    plt.title('MSE2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.gca().set_ylim([0, 0.1])\n",
    "    plt.plot(epochs, var1_values, 'g', label='Var1')\n",
    "    plt.title('Var1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 4)\n",
    "    plt.plot(epochs, var2_values, 'c', label='Var2')\n",
    "    plt.title('Var2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.plot(epochs, entropy1_values, 'm', label='Entropy1')\n",
    "    plt.title('Entropy1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 6)\n",
    "    plt.plot(epochs, entropy2_values, 'y', label='Entropy2')\n",
    "    plt.title('Entropy2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_metrics(s, 'Metrics Over Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728faf81-337b-44c5-8666-bd9bea7cff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/serafim/Desktop/Job/projects/science/hse/GAN-Estimation-Uncertainty/uncertainty/tests/base_gan__lr_0,001_OneCycleLR__2/metrics.pkl', 'wb') as f:\n",
    "    pickle.dump(s, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240443ff-9c63-4238-94c5-f145c2cf3b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/serafim/Desktop/Job/projects/science/hse/GAN-Estimation-Uncertainty/uncertainty/tests/base_gan__variance__10_split__accurate_wieights_0,01_OneCycleLR__lr_0,0001__2/metrics.pkl', 'wb') as f:\n",
    "    pickle.dump(s, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96620dac-2757-4277-882c-88d2c9fd949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('/Users/serafim/Desktop/Job/projects/science/hse/GAN-Estimation-Uncertainty/uncertainty/tests/base_gan__lr_0,001_OneCycleLR__2/metrics.pkl', 'rb') as f:\n",
    "#     ss = pickle.load(f)\n",
    "    \n",
    "# with open('/Users/serafim/Desktop/Job/projects/science/hse/GAN-Estimation-Uncertainty/uncertainty/tests/base_gan__lr_0,001_OneCycleLR__2/metrics.pkl', 'rb') as f:\n",
    "    # mynewlist = pickle.load(f)\n",
    "    \n",
    "filename = '/Users/serafim/Desktop/Job/projects/science/hse/GAN-Estimation-Uncertainty/uncertainty/tests/base_gan__lr_0,001_OneCycleLR__2/metrics.pkl'\n",
    "filename = '/Users/serafim/Desktop/Job/projects/science/hse/GAN-Estimation-Uncertainty/uncertainty/tests/base_gan__variance__10_split__accurate_wieights_0,01_OneCycleLR__lr_0,0001__2/metrics.pkl'\n",
    "filename = '/Users/serafim/Desktop/Job/projects/science/hse/GAN-Estimation-Uncertainty/uncertainty/tests/base_gan__lr_0,001_OneCycleLR__1/metrics.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    classification_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cf4a3a-68f7-4836-98b3-c5c6beb0a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_metrics(ss, 'Variation')\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "# Define a function to plot the metrics\n",
    "def plot_metrics(metrics: List[Tuple[float, float, float, float, float, float]], title: str):\n",
    "    epochs = range(1, len(metrics) + 1)\n",
    "    \n",
    "    mse1_values = [m[0] for m in metrics]\n",
    "    mse2_values = [m[1] for m in metrics]\n",
    "    var1_values = [m[2] for m in metrics]\n",
    "    var2_values = [m[3] for m in metrics]\n",
    "    entropy1_values = [m[4] for m in metrics]\n",
    "    entropy2_values = [m[5] for m in metrics]\n",
    "    \n",
    "    # Plot the metrics\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    # plt.gca().set_ylim([0, 0.1])\n",
    "    plt.plot(epochs, mse1_values, 'g', label='Left variation')\n",
    "    plt.title('Variation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 2)\n",
    "    plt.plot(epochs, mse2_values, 'c', label='Right variation')\n",
    "    # plt.title('Var2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.gca().set_ylim([0, 0.1])\n",
    "    plt.plot(epochs, var1_values, 'g', label='Left variation')\n",
    "    plt.title('Variation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 4)\n",
    "    plt.plot(epochs, var2_values, 'c', label='Right variation')\n",
    "    # plt.title('Var2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    \n",
    "#     plt.subplot(3, 2, 5)\n",
    "#     plt.plot(epochs, entropy1_values, 'm', label='Entropy1')\n",
    "#     plt.title('Entropy1')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.legend()\n",
    "    \n",
    "#     # plt.subplot(3, 2, 6)\n",
    "#     plt.plot(epochs, entropy2_values, 'y', label='Entropy2')\n",
    "#     plt.title('Entropy2')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.legend()\n",
    "    plt.savefig('tests/var_bce_plot.png')\n",
    "#     plt.tight_layout()\n",
    "#     plt.suptitle(title)\n",
    "#     plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_metrics(classification_dict, 'Metrics Over Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd67af2-ff3d-4af7-8839-8f4805ef7030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c60c7c4-46e6-4d91-869c-57e23a8dc41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(ss, 'Metrics Over Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4789116c-e27a-4ad3-a45d-b325f5b771bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to plot the metrics\n",
    "def plot_metrics(metrics: List[Tuple[float, float, float, float, float, float]], title: str):\n",
    "    epochs = range(1, len(metrics) + 1)\n",
    "    \n",
    "    mse1_values = [m[0] for m in metrics]\n",
    "    mse2_values = [m[1] for m in metrics]\n",
    "    var1_values = [m[2] for m in metrics]\n",
    "    var2_values = [m[3] for m in metrics]\n",
    "    entropy1_values = [m[4] for m in metrics]\n",
    "    entropy2_values = [m[5] for m in metrics]\n",
    "    \n",
    "    # Plot the metrics\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "#     plt.subplot(3, 2, 1)\n",
    "#     # plt.gca().set_ylim([0, 0.1])\n",
    "#     plt.plot(epochs, mse1_values, 'b', label='MSE1')\n",
    "#     plt.title('MSE1')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.legend()\n",
    "    \n",
    "#     # plt.subplot(3, 2, 2)\n",
    "#     plt.plot(epochs, mse2_values, 'r', label='MSE2')\n",
    "#     plt.title('MSE2')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    # plt.gca().set_ylim([0, 0.1])\n",
    "    plt.plot(epochs, var1_values, 'g', label='Var1')\n",
    "    # plt.title('Var1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 4)\n",
    "    plt.plot(epochs, var2_values, 'c', label='Var2')\n",
    "    # plt.title('Var2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    \n",
    "    # plt.subplot(1, 2, 1)\n",
    "    plt.gca().set_ylim([0, 0.1])\n",
    "    plt.plot(epochs, var1_values, 'g', label='Var1')\n",
    "    # plt.title('Var1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 4)\n",
    "    plt.plot(epochs, var2_values, 'c', label='Var2')\n",
    "    # plt.title('Var2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()\n",
    "    \n",
    "#     plt.subplot(3, 2, 5)\n",
    "#     plt.plot(epochs, entropy1_values, 'm', label='Entropy1')\n",
    "#     plt.title('Entropy1')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.legend()\n",
    "    \n",
    "#     # plt.subplot(3, 2, 6)\n",
    "#     plt.plot(epochs, entropy2_values, 'y', label='Entropy2')\n",
    "#     plt.title('Entropy2')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.legend()\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    # plt.suptitle(title)\n",
    "    plt.savefig('tests/metrics_bce_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_metrics(ss, 'Train with standart BCE')\n",
    "# plot_metrics(s, 'Train with weighted BCE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27ff306-7f23-4880-8a75-ed81a3985db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4cf937-e4c3-4a90-aaf7-c7b73fb1f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(ss, 'Variances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a0d7f1-bb52-4855-843c-fa5bbf499af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to plot the metrics\n",
    "def plot_metrics(metrics: List[Tuple[float, float, float, float, float, float]],\n",
    "                 metrics2: List[Tuple[float, float, float, float, float, float]],\n",
    "                 title: str):\n",
    "    epochs = range(1, len(metrics) + 1)\n",
    "    \n",
    "    mse1_values = [m[0] for m in metrics]\n",
    "    mse2_values = [m[1] for m in metrics]\n",
    "    var1_values = [m[2] for m in metrics]\n",
    "    var2_values = [m[3] for m in metrics]\n",
    "    entropy1_values = [m[4] for m in metrics]\n",
    "    entropy2_values = [m[5] for m in metrics]\n",
    "    \n",
    "    mse1_values2 = [m[0] for m in metrics2]\n",
    "    mse2_values2 = [m[1] for m in metrics2]\n",
    "    var1_values2 = [m[2] for m in metrics2]\n",
    "    var2_values2 = [m[3] for m in metrics2]\n",
    "    entropy1_values2 = [m[4] for m in metrics2]\n",
    "    entropy2_values2 = [m[5] for m in metrics2]\n",
    "    \n",
    "    # Plot the metrics\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "#     plt.subplot(3, 2, 1)\n",
    "#     # plt.gca().set_ylim([0, 0.1])\n",
    "#     plt.plot(epochs, mse1_values, 'b', label='MSE1')\n",
    "#     plt.title('MSE1')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.legend()\n",
    "    \n",
    "#     # plt.subplot(3, 2, 2)\n",
    "#     plt.plot(epochs, mse2_values, 'r', label='MSE2')\n",
    "#     plt.title('MSE2')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    # plt.gca().set_ylim([0, 0.001])\n",
    "    plt.plot(epochs, var1_values, 'g', label='Left variance')\n",
    "    # plt.title('Var1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 4)\n",
    "    plt.plot(epochs, var2_values, 'c', label='Right variance')\n",
    "    # plt.title('Var2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.gca().set_ylim([0, 0.1])\n",
    "    plt.plot(epochs, var1_values, 'g', label='Left variance')\n",
    "    # plt.title('Var1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 4)\n",
    "    plt.plot(epochs, var2_values, 'c', label='Right variance')\n",
    "    # plt.title('Var2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    \n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.gca().set_ylim([0, 0.1])\n",
    "    \n",
    "    plt.plot(epochs, var1_values2, 'g', label='Left variance')\n",
    "    # plt.title('Var1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 4)\n",
    "    plt.plot(epochs, var2_values2, 'c', label='Right variance')\n",
    "    # plt.title('Var2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    \n",
    "    # plt.subplot(1, 2, 1)\n",
    "    plt.gca().set_ylim([0, 0.1])\n",
    "    \n",
    "    plt.plot(epochs, var1_values2, 'g', label='Left variance')\n",
    "    # plt.title('Var1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 4)\n",
    "    plt.plot(epochs, var2_values2, 'c', label='Right variance')\n",
    "    # plt.title('Var2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.title('Variance')\n",
    "    \n",
    "#     plt.subplot(3, 2, 5)\n",
    "#     plt.plot(epochs, entropy1_values, 'm', label='Entropy1')\n",
    "#     plt.title('Entropy1')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.legend()\n",
    "    \n",
    "#     # plt.subplot(3, 2, 6)\n",
    "#     plt.plot(epochs, entropy2_values, 'y', label='Entropy2')\n",
    "#     plt.title('Entropy2')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.suptitle(title)\n",
    "    # plt.savefig('tests/metrics_bce_wbce_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# plot_metrics(s, 'Variances')\n",
    "\n",
    "plot_metrics(ss, s, 'Variances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce2bb0-f081-4265-96ef-928d9a0d23cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to plot the metrics\n",
    "def plot_metrics(metrics: List[Tuple[float, float, float, float, float, float]],\n",
    "                 metrics2: List[Tuple[float, float, float, float, float, float]],\n",
    "                 title: str):\n",
    "    epochs = range(1, len(metrics) + 1)\n",
    "    \n",
    "    mse1_values = [m[0] for m in metrics]\n",
    "    mse2_values = [m[1] for m in metrics]\n",
    "    var1_values = [m[2] for m in metrics]\n",
    "    var2_values = [m[3] for m in metrics]\n",
    "    entropy1_values = [m[4] for m in metrics]\n",
    "    entropy2_values = [m[5] for m in metrics]\n",
    "    \n",
    "    mse1_values2 = [m[0] for m in metrics2]\n",
    "    mse2_values2 = [m[1] for m in metrics2]\n",
    "    var1_values2 = [m[2] for m in metrics2]\n",
    "    var2_values2 = [m[3] for m in metrics2]\n",
    "    entropy1_values2 = [m[4] for m in metrics2]\n",
    "    entropy2_values2 = [m[5] for m in metrics2]\n",
    "    \n",
    "    # Plot the metrics\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "#     plt.subplot(3, 2, 1)\n",
    "#     # plt.gca().set_ylim([0, 0.1])\n",
    "#     plt.plot(epochs, mse1_values, 'b', label='MSE1')\n",
    "#     plt.title('MSE1')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.legend()\n",
    "    \n",
    "#     # plt.subplot(3, 2, 2)\n",
    "#     plt.plot(epochs, mse2_values, 'r', label='MSE2')\n",
    "#     plt.title('MSE2')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    # plt.gca().set_ylim([0, 0.001])\n",
    "    plt.plot(epochs, var1_values, 'g', label='Left variance BCE')\n",
    "    # plt.title('Var1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 4)\n",
    "    plt.plot(epochs, var1_values2, 'c', label='Left variance wBCE')\n",
    "    # plt.title('Var2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    # plt.gca().set_ylim([0, 0.1])\n",
    "    plt.plot(epochs, var2_values, 'g', label='Right variance BCE')\n",
    "    # plt.title('Var1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 4)\n",
    "    plt.plot(epochs, var2_values2, 'c', label='Right variance wBCE')\n",
    "    # plt.title('Var2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.gca().set_ylim([0, 0.001])\n",
    "    plt.plot(epochs, var1_values, 'g', label='Left variance BCE')\n",
    "    # plt.title('Var1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 4)\n",
    "    plt.plot(epochs, var1_values2, 'c', label='Left variance wBCE')\n",
    "    # plt.title('Var2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.gca().set_ylim([0, 0.1])\n",
    "    plt.plot(epochs, var2_values, 'g', label='Right variance BCE')\n",
    "    # plt.title('Var1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.subplot(3, 2, 4)\n",
    "    plt.plot(epochs, var2_values2, 'c', label='Right variance wBCE')\n",
    "    # plt.title('Var2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plt.title('Variance')\n",
    "    \n",
    "#     plt.subplot(3, 2, 5)\n",
    "#     plt.plot(epochs, entropy1_values, 'm', label='Entropy1')\n",
    "#     plt.title('Entropy1')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.legend()\n",
    "    \n",
    "#     # plt.subplot(3, 2, 6)\n",
    "#     plt.plot(epochs, entropy2_values, 'y', label='Entropy2')\n",
    "#     plt.title('Entropy2')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.suptitle(title)\n",
    "    plt.savefig('tests/metrics1_bce_wbce_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# plot_metrics(s, 'Variances')\n",
    "\n",
    "plot_metrics(ss, s, 'Variances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a427cc7-fb02-4897-a500-443f8bf64919",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e40108-d7bf-4f37-98b5-eb9bf5c9163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/serafim/Desktop/Job/projects/science/hse/GAN-Estimation-Uncertainty/uncertainty/tests/base_gan__variance__10_split__accurate_wieights_0,01_OneCycleLR__lr_0,0001__1/metrics.pkl', 'wb') as f:\n",
    "    pickle.dump(s, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7524e20-a4a4-4805-9f82-f1deebcb9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/2 * np.log(1/2) + 1/2 * np.log(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb55f9a8-1ba6-4573-96fa-fd5a16055e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "3/4 * np.log(3/4) + 1/4 * np.log(1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d78e4-35b1-43b0-ab1d-30eb221ea1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "# loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "loss_function = WeightedVarianceCrossEntropyLoss()\n",
    "save_path = '/Users/serafim/Desktop/Job/projects/science/hse/GAN-Estimation-Uncertainty/uncertainty/tests_range_5_eWBCELoss/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "for i in range(26):\n",
    "    D = Base_Discriminator().to(device=device)\n",
    "    G = Base_Generator().to(device=device)\n",
    "\n",
    "    D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "    G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "    \n",
    "    NAME = f'base_gan__variance__each_point__accurate_wieights_0,01_OneCycleLR__lr_0,0001__{i}'\n",
    "    # Adjust these parameters as needed\n",
    "    max_lr = 0.0005  # Maximum learning rate\n",
    "\n",
    "    # Define the scheduler for both D_optimizer and G_optimizer\n",
    "    scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "    scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "\n",
    "    D_losses_final, G_losses_final, Variances, Weights_variance, s = train(\n",
    "        num_epochs = NUM_EPOCHS,\n",
    "        data_loader = train_loader,\n",
    "        D = D,\n",
    "        G = G,\n",
    "        D_optimizer = D_optimizer,\n",
    "        G_optimizer = G_optimizer, \n",
    "        criterion = loss_function,\n",
    "        device = device,\n",
    "        plot_process = True,\n",
    "        save_path = save_path,\n",
    "        name = f'{NAME}',\n",
    "        weights_interval = True,\n",
    "        # plot_info = True,\n",
    "        animate_bar_var = True,\n",
    "        progress_generator = True,\n",
    "        n_split = 10,\n",
    "        scheduler_D = scheduler_D,\n",
    "        scheduler_G = scheduler_G,\n",
    "        info_n = NUM_EPOCHS,\n",
    "        save_mse = 'G_mse.pkl'\n",
    "    )\n",
    "# plot_sine(G, save_path = save_path, name = NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f537e-9be2-443e-a352-bc98a6b4b567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "# loss_function = nn.BCELoss()\n",
    "\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "# NAME = 'base_gan__lr_0,001_OneCycleLR__1_1'\n",
    "NAME = 'base_gan__variance__10_split__accurate_wieights_0,01_OneCycleLR__lr_0,0001__5_1'\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.0005  # Maximum learning rate\n",
    "\n",
    "# Define the scheduler for both D_optimizer and G_optimizer\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = True,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    n_split = 10,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G,\n",
    "    info_n = 200\n",
    ")\n",
    "# plot_sine(G, save_path = save_path, name = NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49bf2b5-716d-458c-91d6-c7163ae54375",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "# loss_function = nn.BCELoss()\n",
    "\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "# NAME = 'base_gan__lr_0,001_OneCycleLR__1_1'\n",
    "NAME = 'base_gan__variance__10_split__accurate_wieights_0,01_OneCycleLR__lr_0,0001__6_1'\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.0005  # Maximum learning rate\n",
    "\n",
    "# Define the scheduler for both D_optimizer and G_optimizer\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = True,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    n_split = 10,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G,\n",
    "    info_n = 200\n",
    ")\n",
    "# plot_sine(G, save_path = save_path, name = NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3949c3fc-f107-4c0d-bc1d-46a5f2f9f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# D = Base_Discriminator().to(device=device)\n",
    "# G = Base_Generator().to(device=device)\n",
    "# lr = 0.0001\n",
    "# NUM_EPOCHS = 5000\n",
    "# loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "# # loss_function = nn.BCELoss()\n",
    "\n",
    "# D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "# G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# # NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# # NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "# # NAME = 'base_gan__lr_0,001_OneCycleLR__1_1'\n",
    "# NAME = 'base_gan__variance__10_split__accurate_wieights_0,01_OneCycleLR__lr_0,0001__2_1'\n",
    "# # Adjust these parameters as needed\n",
    "# max_lr = 0.0005  # Maximum learning rate\n",
    "\n",
    "# # Define the scheduler for both D_optimizer and G_optimizer\n",
    "# scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "# scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "\n",
    "# D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "#     num_epochs = NUM_EPOCHS,\n",
    "#     data_loader = train_loader,\n",
    "#     D = D,\n",
    "#     G = G,\n",
    "#     D_optimizer = D_optimizer,\n",
    "#     G_optimizer = G_optimizer, \n",
    "#     criterion = loss_function,\n",
    "#     device = device,\n",
    "#     plot_process = True,\n",
    "#     save_path = save_path,\n",
    "#     name = f'{NAME}',\n",
    "#     weights_interval = True,\n",
    "#     # plot_info = True,\n",
    "#     animate_bar_var = True,\n",
    "#     progress_generator = True,\n",
    "#     n_split = 10,\n",
    "#     scheduler_D = scheduler_D,\n",
    "#     scheduler_G = scheduler_G\n",
    "# )\n",
    "# # plot_sine(G, save_path = save_path, name = NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61159e90-388f-45e0-978e-8980cfc03640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6daf545-48a6-4182-9766-7678e1ef515f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "loss_function = nn.BCELoss()\n",
    "max_lr = 0.0005\n",
    "save_path = '/Users/serafim/Desktop/Job/projects/science/hse/GAN-Estimation-Uncertainty/uncertainty/tests_range_2_BCELoss/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    \n",
    "for i in range(50):\n",
    "    D = Base_Discriminator().to(device=device)\n",
    "    G = Base_Generator().to(device=device)\n",
    "\n",
    "    D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "    G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "    NAME = f'base_gan__lr_0,001_OneCycleLR__{i}'\n",
    "\n",
    "# Adjust these parameters as needed\n",
    "\n",
    "\n",
    "    scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "    scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "    D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "        num_epochs = NUM_EPOCHS,\n",
    "        data_loader = train_loader,\n",
    "        D = D,\n",
    "        G = G,\n",
    "        D_optimizer = D_optimizer,\n",
    "        G_optimizer = G_optimizer, \n",
    "        criterion = loss_function,\n",
    "        device = device,\n",
    "        plot_process = True,\n",
    "        save_path = save_path,\n",
    "        name = f'{NAME}',\n",
    "        weights_interval = False,\n",
    "        # plot_info = True,\n",
    "        animate_bar_var = True,\n",
    "        progress_generator = True,\n",
    "        scheduler_D = scheduler_D,\n",
    "        scheduler_G = scheduler_G,\n",
    "        info_n = NUM_EPOCHS,\n",
    "        save_mse = 'G_mse.pkl'\n",
    "        # n_split = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a0c87b-94a3-4f6c-9aec-f0fa5d078fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "wbce_mse_path = 'tests_range_4_WBCELoss/G_mse.pkl'\n",
    "\n",
    "with open(wbce_mse_path, 'rb') as f:\n",
    "    loaded_wbce_G_mse = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ef891-41fd-4477-a09b-b27c791bd2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "ewbce_mse_path = 'tests_range_5_eWBCELoss/G_mse.pkl'\n",
    "\n",
    "with open(ewbce_mse_path, 'rb') as f:\n",
    "    loaded_ewbce_G_mse = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c65ed-eb9e-4ed0-a59f-2f2b0cd77b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_mse_path = 'tests_range_2_BCELoss/G_mse.pkl'\n",
    "\n",
    "with open(bce_mse_path, 'rb') as f:\n",
    "    loaded_bce_G_mse = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f149e72e-8be7-43bb-a4c1-26930a005fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_mse_from_pickle_bar(file_path1: str, file_path2: str) -> None:\n",
    "    \"\"\"\n",
    "    Load and plot MSE values from a pickle file as bar plots.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: Path to the pickle file containing MSE values.\n",
    "    \"\"\"\n",
    "    # Load the MSE values from the pickle file\n",
    "    with open(file_path1, 'rb') as f:\n",
    "        loaded_bce_G_mse = pickle.load(f)\n",
    "    with open(file_path2, 'rb') as f:\n",
    "        loaded_wbce_G_mse = pickle.load(f)\n",
    "    if len(loaded_bce_G_mse) != len(loaded_wbce_G_mse):\n",
    "        min_len = min(len(loaded_bce_G_mse), len(loaded_wbce_G_mse))\n",
    "        \n",
    "        loaded_bce_G_mse = loaded_bce_G_mse[:min_len]\n",
    "        loaded_wbce_G_mse = loaded_wbce_G_mse[:min_len]\n",
    "        print(len(loaded_bce_G_mse), len(loaded_wbce_G_mse))\n",
    "    \n",
    "    # assert(len(loaded_bce_G_mse) == len(loaded_wbce_G_mse))\n",
    "    # Extract MSE values from loaded_G_mse\n",
    "    mse1_values_1 = [mse[0] for mse in loaded_bce_G_mse]\n",
    "    mse2_values_1 = [mse[1] for mse in loaded_bce_G_mse]\n",
    "    \n",
    "    mse1_values_2 = [mse[0] for mse in loaded_wbce_G_mse]\n",
    "    mse2_values_2 = [mse[1] for mse in loaded_wbce_G_mse]\n",
    "\n",
    "    # Calculate average MSE values\n",
    "    avg_mse1_1 = sum(mse1_values_1) / len(mse1_values_1)\n",
    "    avg_mse2_1 = sum(mse2_values_1) / len(mse2_values_1)\n",
    "\n",
    "    avg_mse1_2 = sum(mse1_values_2) / len(mse1_values_2)\n",
    "    avg_mse2_2 = sum(mse2_values_2) / len(mse2_values_2)\n",
    "    \n",
    "\n",
    "    # Plot MSE values as bar plots\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bar_width = 0.35\n",
    "    index = np.arange(len(mse1_values_1))\n",
    "    \n",
    "    \n",
    "    rects1 = ax.bar(index, mse1_values_1, bar_width, label='MSE for left part of sine for bce GAN')\n",
    "    rects2 = ax.bar(index + bar_width, mse1_values_2, bar_width, label='MSE for left part of sine for wbce GAN')\n",
    "    # rects2 = ax.bar(index + bar_width, mse2_values, bar_width, label='MSE for right part of sine')\n",
    "\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.set_title('MSE over iterations')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels([str(i) for i in range(len(mse1_values_1))])\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add text for average MSE values\n",
    "    ax.text(0.5, 0.9, f'Average MSE for left part bce: {avg_mse1_1:.4f}', transform=ax.transAxes, fontsize=10, verticalalignment='top')\n",
    "    ax.text(0.5, 0.85, f'Average MSE for left part wbce: {avg_mse1_2:.4f}', transform=ax.transAxes, fontsize=10, verticalalignment='top')\n",
    "    # ax.text(0.5, 0.85, f'Average MSE for right part: {avg_mse2:.4f}', transform=ax.transAxes, fontsize=10, verticalalignment='top')\n",
    "\n",
    "    \n",
    "    \n",
    "    rects1 = ax.bar(index, mse1_values_1, bar_width, label='MSE for right part of sine for bce GAN')\n",
    "    rects2 = ax.bar(index + bar_width, mse1_values_2, bar_width, label='MSE for right part of sine for wbce GAN')\n",
    "    # rects2 = ax.bar(index + bar_width, mse2_values, bar_width, label='MSE for right part of sine')\n",
    "\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.set_title('MSE over iterations')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels([str(i) for i in range(len(mse2_values_1))])\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add text for average MSE values\n",
    "    ax.text(0.5, 0.9, f'Average MSE for right part bce: {avg_mse2_1:.4f}', transform=ax.transAxes, fontsize=10, verticalalignment='top')\n",
    "    ax.text(0.5, 0.85, f'Average MSE for right part wbce: {avg_mse2_2:.4f}', transform=ax.transAxes, fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_mse_from_pickle_bar(bce_mse_path, ewbce_mse_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a75853-03d8-476e-b981-1c4eea0dfc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_mse_from_pickle_bar(file_path1: str, file_path2: str) -> None:\n",
    "    \"\"\"\n",
    "    Load and plot MSE values from a pickle file as bar plots.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: Path to the pickle file containing MSE values.\n",
    "    \"\"\"\n",
    "    # Load the MSE values from the pickle file\n",
    "    with open(file_path1, 'rb') as f:\n",
    "        loaded_bce_G_mse = pickle.load(f)\n",
    "    with open(file_path2, 'rb') as f:\n",
    "        loaded_wbce_G_mse = pickle.load(f)\n",
    "    assert(len(loaded_bce_G_mse) == len(loaded_wbce_G_mse))\n",
    "    # Extract MSE values from loaded_G_mse\n",
    "    mse1_values_1 = [mse[0] for mse in loaded_bce_G_mse]\n",
    "    mse2_values_1 = [mse[1] for mse in loaded_bce_G_mse]\n",
    "    \n",
    "    mse1_values_2 = [mse[0] for mse in loaded_wbce_G_mse]\n",
    "    mse2_values_2 = [mse[1] for mse in loaded_wbce_G_mse]\n",
    "\n",
    "    # Calculate average MSE values\n",
    "    avg_mse1_1 = sum(mse1_values_1) / len(mse1_values_1)\n",
    "    avg_mse2_1 = sum(mse2_values_1) / len(mse2_values_1)\n",
    "\n",
    "    avg_mse1_2 = sum(mse1_values_2) / len(mse1_values_2)\n",
    "    avg_mse2_2 = sum(mse2_values_2) / len(mse2_values_2)\n",
    "    \n",
    "\n",
    "    # Plot MSE values as bar plots\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bar_width = 0.35\n",
    "    index = np.arange(len(mse1_values_1))\n",
    "    \n",
    "    \n",
    "    rects1 = ax.bar(index, mse1_values_1, bar_width, label='MSE for left part of sine for bce GAN')\n",
    "    rects2 = ax.bar(index + bar_width, mse1_values_2, bar_width, label='MSE for left part of sine for wbce GAN')\n",
    "    # rects2 = ax.bar(index + bar_width, mse2_values, bar_width, label='MSE for right part of sine')\n",
    "\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.set_title('MSE over iterations')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels([str(i) for i in range(len(mse1_values_1))])\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add text for average MSE values\n",
    "    ax.text(0.5, 0.9, f'Average MSE for left part bce: {avg_mse1_1:.4f}', transform=ax.transAxes, fontsize=10, verticalalignment='top')\n",
    "    ax.text(0.5, 0.85, f'Average MSE for left part wbce: {avg_mse1_2:.4f}', transform=ax.transAxes, fontsize=10, verticalalignment='top')\n",
    "    # ax.text(0.5, 0.85, f'Average MSE for right part: {avg_mse2:.4f}', transform=ax.transAxes, fontsize=10, verticalalignment='top')\n",
    "\n",
    "    \n",
    "    \n",
    "    rects1 = ax.bar(index, mse1_values_1, bar_width, label='MSE for right part of sine for bce GAN')\n",
    "    rects2 = ax.bar(index + bar_width, mse1_values_2, bar_width, label='MSE for right part of sine for wbce GAN')\n",
    "    # rects2 = ax.bar(index + bar_width, mse2_values, bar_width, label='MSE for right part of sine')\n",
    "\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.set_title('MSE over iterations')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels([str(i) for i in range(len(mse2_values_1))])\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add text for average MSE values\n",
    "    ax.text(0.5, 0.9, f'Average MSE for right part bce: {avg_mse2_1:.4f}', transform=ax.transAxes, fontsize=10, verticalalignment='top')\n",
    "    ax.text(0.5, 0.85, f'Average MSE for right part wbce: {avg_mse2_2:.4f}', transform=ax.transAxes, fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_mse_from_pickle_bar(bce_mse_path, wbce_mse_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd4dd33-9480-4711-9eed-b5146db7a30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_mse_from_pickle_bar(file_path1: str, file_path2: str) -> None:\n",
    "    \"\"\"\n",
    "    Load and plot MSE values from two pickle files as bar plots.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path1: Path to the first pickle file containing MSE values.\n",
    "    - file_path2: Path to the second pickle file containing MSE values.\n",
    "    \"\"\"\n",
    "    # Load the MSE values from the first pickle file\n",
    "    with open(file_path1, 'rb') as f:\n",
    "        loaded_bce_G_mse = pickle.load(f)\n",
    "        \n",
    "    # Load the MSE values from the second pickle file\n",
    "    with open(file_path2, 'rb') as f:\n",
    "        loaded_wbce_G_mse = pickle.load(f)\n",
    "        \n",
    "    if len(loaded_bce_G_mse) != len(loaded_wbce_G_mse):\n",
    "        min_len = min(len(loaded_bce_G_mse), len(loaded_wbce_G_mse))\n",
    "        \n",
    "        loaded_bce_G_mse = loaded_bce_G_mse[:min_len]\n",
    "        loaded_wbce_G_mse = loaded_wbce_G_mse[:min_len]\n",
    "        print(len(loaded_bce_G_mse), len(loaded_wbce_G_mse))\n",
    "    # assert len(loaded_bce_G_mse) == len(loaded_wbce_G_mse)\n",
    "    \n",
    "    # Extract MSE values for the left and right parts of the sine function\n",
    "    mse1_values_bce = [mse[0] for mse in loaded_bce_G_mse]\n",
    "    mse2_values_bce = [mse[1] for mse in loaded_bce_G_mse]\n",
    "    \n",
    "    mse1_values_wbce = [mse[0] for mse in loaded_wbce_G_mse]\n",
    "    mse2_values_wbce = [mse[1] for mse in loaded_wbce_G_mse]\n",
    "\n",
    "    # Calculate average and median MSE values for both parts\n",
    "    avg_mse1_bce = np.mean(mse1_values_bce)\n",
    "    avg_mse2_bce = np.mean(mse2_values_bce)\n",
    "    median_mse1_bce = np.median(mse1_values_bce)\n",
    "    median_mse2_bce = np.median(mse2_values_bce)\n",
    "    \n",
    "    avg_mse1_wbce = np.mean(mse1_values_wbce)\n",
    "    avg_mse2_wbce = np.mean(mse2_values_wbce)\n",
    "    median_mse1_wbce = np.median(mse1_values_wbce)\n",
    "    median_mse2_wbce = np.median(mse2_values_wbce)\n",
    "\n",
    "    # Plot MSE values as bar plots\n",
    "    fig, axes = plt.subplots(2, figsize=(10, 12))\n",
    "    index = np.arange(len(mse1_values_bce))\n",
    "    bar_width = 0.35\n",
    "\n",
    "    # Plot MSE values for the left part of the sine function\n",
    "    axes[0].bar(index, mse1_values_bce, bar_width, label='MSE for left part of sine for BCE GAN')\n",
    "    axes[0].bar(index + bar_width, mse1_values_wbce, bar_width, label='MSE for left part of sine for WBCE GAN')\n",
    "    axes[0].set_xlabel('Iterations')\n",
    "    axes[0].set_ylabel('MSE')\n",
    "    axes[0].set_title('MSE for Left Part of Sine')\n",
    "    axes[0].set_xticks(index + bar_width / 2)\n",
    "    axes[0].set_xticklabels([str(i) for i in range(len(mse1_values_bce))])\n",
    "    axes[0].legend()\n",
    "    print(f'The left part:\\n Mean MSE: BCE GAN -- {avg_mse1_bce:.5f}, WBCE GAN -- {avg_mse1_wbce:.5f},\\n Median MSE: BCE GAN -- {median_mse1_bce:.5f}, WBCE GAN -- {median_mse1_wbce:.5f}')\n",
    "    # axes[0].text(0.5, 0.9, f'Average MSE for left part BCE: {avg_mse1_bce:.4f}', transform=axes[0].transAxes, fontsize=10, verticalalignment='top')\n",
    "    # axes[0].text(0.5, 0.85, f'Median MSE for left part BCE: {median_mse1_bce:.4f}', transform=axes[0].transAxes, fontsize=10, verticalalignment='top')\n",
    "    # axes[0].text(0.5, 0.8, f'Average MSE for left part WBCE: {avg_mse1_wbce:.4f}', transform=axes[0].transAxes, fontsize=10, verticalalignment='top')\n",
    "    # axes[0].text(0.5, 0.75, f'Median MSE for left part WBCE: {median_mse1_wbce:.4f}', transform=axes[0].transAxes, fontsize=10, verticalalignment='top')\n",
    "\n",
    "    # Plot MSE values for the right part of the sine function\n",
    "    axes[1].bar(index, mse2_values_bce, bar_width, label='MSE for right part of sine for BCE GAN')\n",
    "    axes[1].bar(index + bar_width, mse2_values_wbce, bar_width, label='MSE for right part of sine for WBCE GAN')\n",
    "    axes[1].set_xlabel('Iterations')\n",
    "    axes[1].set_ylabel('MSE')\n",
    "    axes[1].set_title('MSE for Right Part of Sine')\n",
    "    axes[1].set_xticks(index + bar_width / 2)\n",
    "    axes[1].set_xticklabels([str(i) for i in range(len(mse2_values_bce))])\n",
    "    axes[1].legend()\n",
    "    # axes[1].text(0.5, 0.9, f'Average MSE for right part BCE: {avg_mse2_bce:.4f}', transform=axes[1].transAxes, fontsize=10, verticalalignment='top')\n",
    "    # axes[1].text(0.5, 0.85, f'Median MSE for right part BCE: {median_mse2_bce:.4f}', transform=axes[1].transAxes, fontsize=10, verticalalignment='top')\n",
    "    # axes[1].text(0.5, 0.8, f'Average MSE for right part WBCE: {avg_mse2_wbce:.4f}', transform=axes[1].transAxes, fontsize=10, verticalalignment='top')\n",
    "    # axes[1].text(0.5, 0.75, f'Median MSE for right part WBCE: {median_mse2_wbce:.4f}', transform=axes[1].transAxes, fontsize=10, verticalalignment='top')\n",
    "    print(f'\\nThe right part:\\n Mean MSE: BCE GAN -- {avg_mse2_bce:.5f}, WBCE GAN -- {avg_mse2_wbce:.5f},\\n Median MSE: BCE GAN -- {median_mse2_bce:.5f}, WBCE GAN -- {median_mse2_wbce:.5f}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_mse_from_pickle_bar(bce_mse_path, ewbce_mse_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485efd92-0161-411d-8ec5-1da65ab4d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_mse_from_pickle_bar(file_path1: str, file_path2: str) -> None:\n",
    "    \"\"\"\n",
    "    Load and plot MSE values from two pickle files as bar plots.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path1: Path to the first pickle file containing MSE values.\n",
    "    - file_path2: Path to the second pickle file containing MSE values.\n",
    "    \"\"\"\n",
    "    # Load the MSE values from the first pickle file\n",
    "    with open(file_path1, 'rb') as f:\n",
    "        loaded_bce_G_mse = pickle.load(f)\n",
    "        \n",
    "    # Load the MSE values from the second pickle file\n",
    "    with open(file_path2, 'rb') as f:\n",
    "        loaded_wbce_G_mse = pickle.load(f)\n",
    "        \n",
    "    assert len(loaded_bce_G_mse) == len(loaded_wbce_G_mse)\n",
    "    \n",
    "    # Extract MSE values for the left and right parts of the sine function\n",
    "    mse1_values_bce = [mse[0] for mse in loaded_bce_G_mse]\n",
    "    mse2_values_bce = [mse[1] for mse in loaded_bce_G_mse]\n",
    "    \n",
    "    mse1_values_wbce = [mse[0] for mse in loaded_wbce_G_mse]\n",
    "    mse2_values_wbce = [mse[1] for mse in loaded_wbce_G_mse]\n",
    "\n",
    "    # Calculate average and median MSE values for both parts\n",
    "    avg_mse1_bce = np.mean(mse1_values_bce)\n",
    "    avg_mse2_bce = np.mean(mse2_values_bce)\n",
    "    median_mse1_bce = np.median(mse1_values_bce)\n",
    "    median_mse2_bce = np.median(mse2_values_bce)\n",
    "    \n",
    "    avg_mse1_wbce = np.mean(mse1_values_wbce)\n",
    "    avg_mse2_wbce = np.mean(mse2_values_wbce)\n",
    "    median_mse1_wbce = np.median(mse1_values_wbce)\n",
    "    median_mse2_wbce = np.median(mse2_values_wbce)\n",
    "\n",
    "    # Plot MSE values as bar plots\n",
    "    fig, axes = plt.subplots(2, figsize=(10, 12))\n",
    "    index = np.arange(len(mse1_values_bce))\n",
    "    bar_width = 0.35\n",
    "\n",
    "    # Plot MSE values for the left part of the sine function\n",
    "    axes[0].bar(index, mse1_values_bce, bar_width, label='MSE for left part of sine for BCE GAN')\n",
    "    axes[0].bar(index + bar_width, mse1_values_wbce, bar_width, label='MSE for left part of sine for WBCE GAN')\n",
    "    axes[0].set_xlabel('Iterations')\n",
    "    axes[0].set_ylabel('MSE')\n",
    "    axes[0].set_title('MSE for Left Part of Sine')\n",
    "    axes[0].set_xticks(index + bar_width / 2)\n",
    "    axes[0].set_xticklabels([str(i) for i in range(len(mse1_values_bce))])\n",
    "    axes[0].legend()\n",
    "    print(f'The left part:\\n Mean MSE: BCE GAN -- {avg_mse1_bce:.5f}, WBCE GAN -- {avg_mse1_wbce:.5f},\\n Median MSE: BCE GAN -- {median_mse1_bce:.5f}, WBCE GAN -- {median_mse1_wbce:.5f}')\n",
    "    # axes[0].text(0.5, 0.9, f'Average MSE for left part BCE: {avg_mse1_bce:.4f}', transform=axes[0].transAxes, fontsize=10, verticalalignment='top')\n",
    "    # axes[0].text(0.5, 0.85, f'Median MSE for left part BCE: {median_mse1_bce:.4f}', transform=axes[0].transAxes, fontsize=10, verticalalignment='top')\n",
    "    # axes[0].text(0.5, 0.8, f'Average MSE for left part WBCE: {avg_mse1_wbce:.4f}', transform=axes[0].transAxes, fontsize=10, verticalalignment='top')\n",
    "    # axes[0].text(0.5, 0.75, f'Median MSE for left part WBCE: {median_mse1_wbce:.4f}', transform=axes[0].transAxes, fontsize=10, verticalalignment='top')\n",
    "\n",
    "    # Plot MSE values for the right part of the sine function\n",
    "    axes[1].bar(index, mse2_values_bce, bar_width, label='MSE for right part of sine for BCE GAN')\n",
    "    axes[1].bar(index + bar_width, mse2_values_wbce, bar_width, label='MSE for right part of sine for WBCE GAN')\n",
    "    axes[1].set_xlabel('Iterations')\n",
    "    axes[1].set_ylabel('MSE')\n",
    "    axes[1].set_title('MSE for Right Part of Sine')\n",
    "    axes[1].set_xticks(index + bar_width / 2)\n",
    "    axes[1].set_xticklabels([str(i) for i in range(len(mse2_values_bce))])\n",
    "    axes[1].legend()\n",
    "    # axes[1].text(0.5, 0.9, f'Average MSE for right part BCE: {avg_mse2_bce:.4f}', transform=axes[1].transAxes, fontsize=10, verticalalignment='top')\n",
    "    # axes[1].text(0.5, 0.85, f'Median MSE for right part BCE: {median_mse2_bce:.4f}', transform=axes[1].transAxes, fontsize=10, verticalalignment='top')\n",
    "    # axes[1].text(0.5, 0.8, f'Average MSE for right part WBCE: {avg_mse2_wbce:.4f}', transform=axes[1].transAxes, fontsize=10, verticalalignment='top')\n",
    "    # axes[1].text(0.5, 0.75, f'Median MSE for right part WBCE: {median_mse2_wbce:.4f}', transform=axes[1].transAxes, fontsize=10, verticalalignment='top')\n",
    "    print(f'\\nThe right part:\\n Mean MSE: BCE GAN -- {avg_mse2_bce:.5f}, WBCE GAN -- {avg_mse2_wbce:.5f},\\n Median MSE: BCE GAN -- {median_mse2_bce:.5f}, WBCE GAN -- {median_mse2_wbce:.5f}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_mse_from_pickle_bar(bce_mse_path, wbce_mse_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c904612-85c2-4eed-97b5-079c01817b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b35eb7-eb81-4066-8298-59d520bc904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn==1.2.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301dd49-4835-4efa-86e0-e28aee688c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# file_path = 'G_mse.pkl'\n",
    "# file_path = os.path.join(save_path, file_path)\n",
    "\n",
    "bce_mse_path = 'tests_range_2_BCELoss/G_mse.pkl'\n",
    "wbce_mse_path = 'tests_range_4_WBCELoss/G_mse.pkl'\n",
    "\n",
    "def plot_mse_from_pickle(file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Load and plot MSE values from a pickle file.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: Path to the pickle file containing MSE values.\n",
    "    \"\"\"\n",
    "    # Load the MSE values from the pickle file\n",
    "    with open(file_path, 'rb') as f:\n",
    "        loaded_G_mse = pickle.load(f)\n",
    "    # print(loaded_G_mse)\n",
    "\n",
    "    # Extract MSE values from loaded_G_mse\n",
    "    mse1_values = [mse[0] for mse in loaded_G_mse]\n",
    "    mse2_values = [mse[1] for mse in loaded_G_mse]\n",
    "\n",
    "    # Calculate average MSE values\n",
    "    avg_mse1 = sum(mse1_values) / len(mse1_values)\n",
    "    avg_mse2 = sum(mse2_values) / len(mse2_values)\n",
    "\n",
    "    # Plot MSE values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(mse1_values, label='MSE for left part of sine')\n",
    "    plt.plot(mse2_values, label='MSE for right part of sine')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title('MSE over iterations')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Add text for average MSE values\n",
    "    plt.text(0.5, 0.9, f'Average MSE for left part: {avg_mse1:.4f}', transform=plt.gca().transAxes, fontsize=10, verticalalignment='top')\n",
    "    plt.text(0.5, 0.85, f'Average MSE for right part: {avg_mse2:.4f}', transform=plt.gca().transAxes, fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_mse_from_pickle(wbce_mse_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a634a09-bfcb-447a-a337-395d037e53d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_mse_from_pickle_bar(file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Load and plot MSE values from a pickle file as bar plots.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: Path to the pickle file containing MSE values.\n",
    "    \"\"\"\n",
    "    # Load the MSE values from the pickle file\n",
    "    with open(file_path, 'rb') as f:\n",
    "        loaded_G_mse = pickle.load(f)\n",
    "\n",
    "    # Extract MSE values from loaded_G_mse\n",
    "    mse1_values = [mse[0] for mse in loaded_G_mse]\n",
    "    mse2_values = [mse[1] for mse in loaded_G_mse]\n",
    "\n",
    "    # Calculate average MSE values\n",
    "    avg_mse1 = sum(mse1_values) / len(mse1_values)\n",
    "    avg_mse2 = sum(mse2_values) / len(mse2_values)\n",
    "\n",
    "    # Plot MSE values as bar plots\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bar_width = 0.35\n",
    "    index = np.arange(len(mse1_values))\n",
    "    rects1 = ax.bar(index, mse1_values, bar_width, label='MSE for left part of sine')\n",
    "    rects2 = ax.bar(index + bar_width, mse2_values, bar_width, label='MSE for right part of sine')\n",
    "\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.set_title('MSE over iterations')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels([str(i) for i in range(len(mse1_values))])\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add text for average MSE values\n",
    "    ax.text(0.5, 0.9, f'Average MSE for left part: {avg_mse1:.4f}', transform=ax.transAxes, fontsize=10, verticalalignment='top')\n",
    "    ax.text(0.5, 0.85, f'Average MSE for right part: {avg_mse2:.4f}', transform=ax.transAxes, fontsize=10, verticalalignment='top')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_mse_from_pickle_bar(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4fa35a-3b67-4fb6-bff1-b7c4cc7a4db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_mse_from_pickle_bar(file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Load and plot MSE values from a pickle file as bar plots.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: Path to the pickle file containing MSE values.\n",
    "    \"\"\"\n",
    "    # Load the MSE values from the pickle file\n",
    "    with open(file_path, 'rb') as f:\n",
    "        loaded_G_mse = pickle.load(f)\n",
    "\n",
    "    # Extract MSE values from loaded_G_mse\n",
    "    mse1_values = [mse[0] for mse in loaded_G_mse]\n",
    "    mse2_values = [mse[1] for mse in loaded_G_mse]\n",
    "\n",
    "    # Calculate average MSE values\n",
    "    avg_mse1 = sum(mse1_values) / len(mse1_values)\n",
    "    avg_mse2 = sum(mse2_values) / len(mse2_values)\n",
    "\n",
    "    # Plot MSE values as bar plots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    index = np.arange(len(mse1_values))\n",
    "    bar_width = 0.35\n",
    "\n",
    "    rects1 = ax1.bar(index, mse1_values, bar_width, label='MSE for left part of sine', color='b')\n",
    "    ax1.set_ylabel('MSE')\n",
    "    ax1.set_title('MSE for left part of sine')\n",
    "    ax1.set_xticks(index)\n",
    "    ax1.set_xticklabels([str(i) for i in range(len(mse1_values))])\n",
    "    ax1.legend()\n",
    "    \n",
    "    rects2 = ax2.bar(index, mse2_values, bar_width, label='MSE for right part of sine', color='r')\n",
    "    ax2.set_ylabel('MSE')\n",
    "    ax2.set_title('MSE for right part of sine')\n",
    "    ax2.set_xticks(index)\n",
    "    ax2.set_xticklabels([str(i) for i in range(len(mse2_values))])\n",
    "    ax2.legend()\n",
    "\n",
    "    # Add text for average MSE values\n",
    "    ax1.text(0.5, 0.9, f'Average MSE for left part: {avg_mse1:.4f}', transform=ax1.transAxes, fontsize=10, verticalalignment='top', color='b')\n",
    "    ax2.text(0.5, 0.9, f'Average MSE for right part: {avg_mse2:.4f}', transform=ax2.transAxes, fontsize=10, verticalalignment='top', color='r')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_mse_from_pickle_bar(bce_mse_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bfd6df-8507-4f8c-b9c9-ac1a02996848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_mse_from_pickle_bar(file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Load and plot MSE values from a pickle file as bar plots, with rects1 on the left and rects2 on the right.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: Path to the pickle file containing MSE values.\n",
    "    \"\"\"\n",
    "    # Load the MSE values from the pickle file\n",
    "    with open(file_path, 'rb') as f:\n",
    "        loaded_G_mse = pickle.load(f)\n",
    "\n",
    "    # Extract MSE values from loaded_G_mse\n",
    "    mse1_values = [mse[0] for mse in loaded_G_mse]\n",
    "    mse2_values = [mse[1] for mse in loaded_G_mse]\n",
    "\n",
    "    # Calculate average MSE values\n",
    "    avg_mse1 = sum(mse1_values) / len(mse1_values)\n",
    "    avg_mse2 = sum(mse2_values) / len(mse2_values)\n",
    "\n",
    "    # Plot MSE values as bar plots with rects1 on the left and rects2 on the right\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bar_width = 0.35\n",
    "    index = np.arange(len(mse1_values))\n",
    "    rects1 = ax.bar(index, mse1_values, bar_width, label='MSE for left part of sine', align='edge')\n",
    "    rects2 = ax.bar(index + bar_width, mse2_values, bar_width, label='MSE for right part of sine', align='edge')\n",
    "\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.set_title('MSE over iterations')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels([str(i) for i in range(len(mse1_values))])\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add text for average MSE values\n",
    "    ax.text(0.5, 0.9, f'Average MSE for left part: {avg_mse1:.4f}', transform=ax.transAxes, fontsize=10, verticalalignment='top')\n",
    "    ax.text(0.5, 0.85, f'Average MSE for right part: {avg_mse2:.4f}', transform=ax.transAxes, fontsize=10, verticalalignment='top')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_mse_from_pickle_bar(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f28e90a-3b80-4912-8813-a3d4a06454aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_mse_distribution(file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Load and plot the distribution of MSE values from a pickle file as histograms, \n",
    "    with rects1 on the left and rects2 on the right.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: Path to the pickle file containing MSE values.\n",
    "    \"\"\"\n",
    "    # Load the MSE values from the pickle file\n",
    "    with open(file_path, 'rb') as f:\n",
    "        loaded_G_mse = pickle.load(f)\n",
    "\n",
    "    # Extract MSE values from loaded_G_mse\n",
    "    mse1_values = [mse[0] for mse in loaded_G_mse]\n",
    "    mse2_values = [mse[1] for mse in loaded_G_mse]\n",
    "\n",
    "    # Calculate average MSE values\n",
    "    avg_mse1 = sum(mse1_values) / len(mse1_values)\n",
    "    avg_mse2 = sum(mse2_values) / len(mse2_values)\n",
    "\n",
    "    # Plot MSE distributions as histograms with rects1 on the left and rects2 on the right\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "    \n",
    "    axs[0].hist(mse1_values, bins=2, color='blue', alpha=0.7, label='MSE for left part of sine')\n",
    "    axs[1].hist(mse2_values, bins=2, color='orange', alpha=0.7, label='MSE for right part of sine')\n",
    "\n",
    "    axs[0].set_xlabel('MSE')\n",
    "    axs[0].set_ylabel('Frequency')\n",
    "    axs[0].set_title('Distribution of MSE for left part')\n",
    "    axs[1].set_xlabel('MSE')\n",
    "    axs[1].set_title('Distribution of MSE for right part')\n",
    "    \n",
    "    # Add text for average MSE values\n",
    "    axs[0].text(0.5, 0.9, f'Average MSE: {avg_mse1:.4f}', transform=axs[0].transAxes, fontsize=10, verticalalignment='top')\n",
    "    axs[1].text(0.5, 0.9, f'Average MSE: {avg_mse2:.4f}', transform=axs[1].transAxes, fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_mse_distribution(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd7025-98d8-45bd-a2ec-048b74348fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "# loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "loss_function = nn.BCELoss()\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "NAME = 'base_gan__lr_0,001_OneCycleLR__8_1'\n",
    "\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.0005\n",
    "\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = False,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G,\n",
    "    info_n = 200\n",
    "    # n_split = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc34e211-1328-413a-8d92-719f43f1a4b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "# loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "loss_function = nn.BCELoss()\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "NAME = 'base_gan__lr_0,001_OneCycleLR__15_1'\n",
    "\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.0005\n",
    "\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = False,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G,\n",
    "    info_n = 200\n",
    "    # n_split = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d2ab50-14e5-4ee6-9908-c6b80e8d51f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "# loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "loss_function = nn.BCELoss()\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "NAME = 'base_gan__lr_0,001_OneCycleLR__16_1'\n",
    "\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.0005\n",
    "\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = False,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G,\n",
    "    info_n = 200\n",
    "    # n_split = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc96b3e2-184a-4c9b-8071-6aaeacfda7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "# loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "loss_function = nn.BCELoss()\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "NAME = 'base_gan__lr_0,001_OneCycleLR__17_1'\n",
    "\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.0005\n",
    "\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = False,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G,\n",
    "    info_n = 200\n",
    "    # n_split = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd0c2c3-450b-490b-a540-c44201786c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "# loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "loss_function = nn.BCELoss()\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "NAME = 'base_gan__lr_0,001_OneCycleLR__18_1'\n",
    "\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.0005\n",
    "\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = False,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G,\n",
    "    info_n = 200\n",
    "    # n_split = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e8b4a6-c89d-489c-b6ea-5615fbf76f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "# loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "loss_function = nn.BCELoss()\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "NAME = 'base_gan__lr_0,001_OneCycleLR__9_1'\n",
    "\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.0005\n",
    "\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = False,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G,\n",
    "    info_n = 200\n",
    "    # n_split = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8d3f71-a254-4629-b94b-60e5cd6490e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "# loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "loss_function = nn.BCELoss()\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "NAME = 'base_gan__lr_0,001_OneCycleLR__10_1'\n",
    "\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.0005\n",
    "\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = False,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G,\n",
    "    info_n = 200\n",
    "    # n_split = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a5ef2b-8d39-4f0f-ba0b-a76c8c666726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb673a-4de6-4bf8-bc8a-b65cd3a61303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "# loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "loss_function = nn.BCELoss()\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "NAME = 'base_gan__lr_0,001_OneCycleLR__11_1'\n",
    "\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.0005\n",
    "\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = False,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G,\n",
    "    info_n = 200\n",
    "    # n_split = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ccdcb-7100-4f79-8ccb-3f88a5fca77d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "# loss_function = nn.BCELoss()\n",
    "\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "# NAME = 'base_gan__lr_0,001_OneCycleLR__1_1'\n",
    "NAME = 'base_gan__variance__2_split__accurate_wieights_0,01_OneCycleLR__lr_0,0001__12_1'\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.0005  # Maximum learning rate\n",
    "\n",
    "# Define the scheduler for both D_optimizer and G_optimizer\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = True,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    n_split = 2,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G,\n",
    "    info_n = 200\n",
    ")\n",
    "# plot_sine(G, save_path = save_path, name = NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f242ed68-7c9c-442b-b1f2-3201bbe4fb5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "# loss_function = nn.BCELoss()\n",
    "\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "# NAME = 'base_gan__lr_0,001_OneCycleLR__1_1'\n",
    "NAME = 'base_gan__variance__6_split__accurate_wieights_0,01_OneCycleLR__lr_0,0001__13_1'\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.0005  # Maximum learning rate\n",
    "\n",
    "# Define the scheduler for both D_optimizer and G_optimizer\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = True,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    n_split = 6,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G,\n",
    "    info_n = 200\n",
    ")\n",
    "# plot_sine(G, save_path = save_path, name = NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aec09e-9358-4934-b3d3-875e1d59063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "# loss_function = nn.BCELoss()\n",
    "\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "# NAME = 'base_gan__lr_0,001_OneCycleLR__1_1'\n",
    "NAME = 'base_gan__variance__20_split__accurate_wieights_0,01_OneCycleLR__lr_0,0001__14_1'\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.0005  # Maximum learning rate\n",
    "\n",
    "# Define the scheduler for both D_optimizer and G_optimizer\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = True,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    n_split = 20,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G,\n",
    "    info_n = 200\n",
    ")\n",
    "# plot_sine(G, save_path = save_path, name = NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30368e79-09bd-4d4d-be77-b679cadb4d75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 1000\n",
    "loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "# loss_function = nn.BCELoss()\n",
    "\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "# NAME = 'base_gan__lr_0,001_OneCycleLR__1_1'\n",
    "NAME = 'base_gan__variance__10_split__accurate_wieights_0,01_OneCycleLR__lr_0,001__3_1'\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.0005  # Maximum learning rate\n",
    "\n",
    "# Define the scheduler for both D_optimizer and G_optimizer\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = True,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    n_split = 4,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G\n",
    ")\n",
    "# plot_sine(G, save_path = save_path, name = NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea672e-8e87-4477-a015-cf695d3172a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 1000\n",
    "loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "# loss_function = nn.BCELoss()\n",
    "\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "# NAME = 'base_gan__lr_0,001_OneCycleLR__1_1'\n",
    "NAME = 'base_gan__variance_std__2_split__accurate_wieights_0,01_OneCycleLR__lr_0,001__4_1'\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.0005  # Maximum learning rate\n",
    "\n",
    "# Define the scheduler for both D_optimizer and G_optimizer\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = True,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    n_split = 2,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G\n",
    ")\n",
    "# plot_sine(G, save_path = save_path, name = NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930ff25e-7028-4948-864c-296830446af5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 1000\n",
    "loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "# loss_function = nn.BCELoss()\n",
    "\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "# NAME = 'base_gan__lr_0,001_OneCycleLR__1_1'\n",
    "NAME = 'base_gan__variance_std__25_split__accurate_wieights_0,01_OneCycleLR__lr_0,001__5_1'\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.0005  # Maximum learning rate\n",
    "\n",
    "# Define the scheduler for both D_optimizer and G_optimizer\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = True,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    n_split = 25,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G\n",
    ")\n",
    "# plot_sine(G, save_path = save_path, name = NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2bad18-23d1-4319-840c-5181a9369784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 5000\n",
    "loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "# loss_function = nn.BCELoss()\n",
    "\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "# NAME = 'base_gan__lr_0,001_OneCycleLR__1_1'\n",
    "NAME = 'base_gan__variance__10_split__accurate_wieights_0,01_OneCycleLR__lr_0,001__3_1'\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.0005  # Maximum learning rate\n",
    "\n",
    "# Define the scheduler for both D_optimizer and G_optimizer\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = True,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    n_split = 10,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G\n",
    ")\n",
    "# plot_sine(G, save_path = save_path, name = NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea26b6fe-d45e-4920-97e7-ab0f991b6d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 1000\n",
    "# loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "loss_function = nn.BCELoss()\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "NAME = 'base_gan__lr_0,001_OneCycleLR__6_1'\n",
    "\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.001  # Maximum learning rate\n",
    "\n",
    "# Define the scheduler for both D_optimizer and G_optimizer\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = False,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G\n",
    "    # n_split = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4094e65f-0ca0-4955-a09d-b090e6810a84",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 1000\n",
    "loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "# loss_function = nn.BCELoss()\n",
    "\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "# NAME = 'base_gan__lr_0,001_OneCycleLR__1_1'\n",
    "NAME = 'base_gan__variance_10_split__accurate_wieights_0,01_OneCycleLR__lr_0,001__3_1'\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.001  # Maximum learning rate\n",
    "\n",
    "# Define the scheduler for both D_optimizer and G_optimizer\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = True,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    # scheduler_D = scheduler_D,\n",
    "    # scheduler_G = scheduler_G\n",
    "    n_split = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf8ffc-613a-428f-9f23-447576a5c0f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 1000\n",
    "# loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "loss_function = nn.BCELoss()\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "NAME = 'base_gan__part_dataset__lr_0,001_OneCycleLR__10_1'\n",
    "\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.001  # Maximum learning rate\n",
    "\n",
    "# Define the scheduler for both D_optimizer and G_optimizer\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = True,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    n_split = 10,\n",
    "    scheduler_D = scheduler_D,\n",
    "    scheduler_G = scheduler_G\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e91ad9-dad6-4846-95b7-c29c0136b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 1000\n",
    "# loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "loss_function = nn.BCELoss()\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "NAME = 'base_gan__lr_0,001_OneCycleLR__6_1'\n",
    "\n",
    "# Adjust these parameters as needed\n",
    "max_lr = 0.001  # Maximum learning rate\n",
    "\n",
    "# Define the scheduler for both D_optimizer and G_optimizer\n",
    "scheduler_D = OneCycleLR(D_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "scheduler_G = OneCycleLR(G_optimizer, max_lr=max_lr, total_steps=NUM_EPOCHS * NUM_BATCHES)\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = False,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    # n_split = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb82d9-f2fb-41c1-a0aa-18284648f889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 1000\n",
    "loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "NAME = 'base_gan__variance_10_split__accurate_wieights_0,01__lr_0,0001__3_1'\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = True,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    n_split = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f679259b-6bd0-4342-92b2-f2edd46737f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.0001\n",
    "NUM_EPOCHS = 1000\n",
    "loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "# NAME = 'base_gan__variance_2_split__accurate_wieights__4_1'\n",
    "# NAME = 'base_gan__variance_10_split__accurate_wieights__lr_0,0001__11_1'\n",
    "NAME = 'base_gan__variance_10_split__accurate_wieights_0,01__lr_0,0001__4_1'\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = True,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    n_split = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ed9db-575f-4b1c-bffd-e853d6e3553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.001\n",
    "NUM_EPOCHS = 1000\n",
    "loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "NAME = 'base_gan__variance_20_split__accurate_wieights__5_1'\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = True,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    n_split = 20\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378bfac7-ad80-42df-9de5-72351106ea5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.001\n",
    "NUM_EPOCHS = 1000\n",
    "loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "NAME = 'base_gan__variance_50_split__accurate_wieights__6_1'\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = True,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    n_split = 50\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e2157f-f4c0-4e4d-8fb4-8ee39cbafeeb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.001\n",
    "NUM_EPOCHS = 1000\n",
    "loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "NAME = 'base_gan__variance_100_split__accurate_wieights__7_1'\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = True,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    n_split = 100\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6024a2ab-0c91-4f34-a011-e23f7e67c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D = Base_Discriminator().to(device=device)\n",
    "G = Base_Generator().to(device=device)\n",
    "lr = 0.001\n",
    "NUM_EPOCHS = 1000\n",
    "loss_function = WeightedIntervalCrossEntropyLoss()\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "NAME = 'base_gan__variance_200_split__accurate_wieights__8_1'\n",
    "\n",
    "D_losses_final, G_losses_final, Variances, Weights_variance = train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = train_loader,\n",
    "    D = D,\n",
    "    G = G,\n",
    "    D_optimizer = D_optimizer,\n",
    "    G_optimizer = G_optimizer, \n",
    "    criterion = loss_function,\n",
    "    device = device,\n",
    "    plot_process = True,\n",
    "    save_path = save_path,\n",
    "    name = f'{NAME}',\n",
    "    weights_interval = True,\n",
    "    # plot_info = True,\n",
    "    animate_bar_var = True,\n",
    "    progress_generator = True,\n",
    "    n_split = 200\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402be4b7-e77f-4312-a399-ad82a78d7529",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb82731f-a8a4-4357-8a75-88bb508e1f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_gif(file_paths, gif_path, save_path, name, duration=NUM_EPOCHS, loop=0):\n",
    "    images = [plt.imread(file_path) for file_path in file_paths]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.axis('off')\n",
    "    ims = plt.imshow(images[0])\n",
    "\n",
    "    def update(i):\n",
    "        ims.set_array(images[i])\n",
    "\n",
    "    ani = FuncAnimation(plt.gcf(), update, frames=len(images), interval=duration, repeat_delay=1000)\n",
    "    \n",
    "    gif_path = os.path.join(save_path, NAME, 'final_generated_plots.gif')\n",
    "    ani.save(gif_path, writer='imagemagick', fps=2, dpi=80)\n",
    "    \n",
    "    # Delete all files except the last epoch\n",
    "    for file_path in file_paths[:-1]:\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1857cf8f-25cb-4eae-875f-83b2aea006c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179595c8-54e7-4ac5-85cb-240584b73843",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [os.path.join(save_path, NAME, f'generated_plots_epoch_{epoch}.png') for epoch in range(0, 40, 20)]\n",
    "create_gif(file_paths, save_path = save_path, name = NAME, gif_path='final_generated_plots.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a8cbc0-5aa8-4980-9c21-0f7ff4c53be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b49428a-e366-4633-bae7-e603bc460e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8065f304-708c-4cbc-ae29-5a063a6e567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_EXPERIMENTS = 10\n",
    "# NUM_EPOCHS = 20\n",
    "\n",
    "# # Initialize lists to store results for each experiment\n",
    "# all_D_losses_final = []\n",
    "# all_G_losses_final = []\n",
    "# all_Variances = []\n",
    "\n",
    "# for experiment in range(NUM_EXPERIMENTS):\n",
    "#     # Initialize your models, optimizers, etc. here if needed\n",
    "    \n",
    "#     # Run the training process\n",
    "#     D_losses_final, G_losses_final, Variances = train(\n",
    "#         num_epochs=NUM_EPOCHS,\n",
    "#         data_loader=train_loader,\n",
    "#         D=D,\n",
    "#         G=G,\n",
    "#         D_optimizer=D_optimizer,\n",
    "#         G_optimizer=G_optimizer,\n",
    "#         criterion=loss_function,\n",
    "#         device=device,\n",
    "#         plot_process=False,  # Disable plotting for individual experiments\n",
    "#         save_path=save_path,\n",
    "#         name=f'{NAME}_experiment_{experiment}',\n",
    "#         weights_interval=True\n",
    "#     )\n",
    "    \n",
    "#     plot_sine(G, save_path = save_path, name = NAME)\n",
    "    \n",
    "#     # Store results for this experiment\n",
    "#     all_D_losses_final.append(D_losses_final)\n",
    "#     all_G_losses_final.append(G_losses_final)\n",
    "#     all_Variances.append(Variances)\n",
    "\n",
    "# # After all experiments, you can analyze the aggregated results or perform any desired post-processing.\n",
    "\n",
    "# # For example, you can print the average and standard deviation of losses across experiments:\n",
    "# avg_D_losses_final = sum(all_D_losses_final) / NUM_EXPERIMENTS\n",
    "# avg_G_losses_final = sum(all_G_losses_final) / NUM_EXPERIMENTS\n",
    "# avg_Variances = sum(all_Variances) / NUM_EXPERIMENTS\n",
    "\n",
    "# print(f'Average D_losses_final: {avg_D_losses_final}')\n",
    "# print(f'Average G_losses_final: {avg_G_losses_final}')\n",
    "# print(f'Average Variances: {avg_Variances}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097b200-2336-43a8-8607-3a10946d9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sine(G, save_path = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0ccdb5-3d2a-46fd-8070-4802abeb05f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [el[0] for el in vals]\n",
    "['Category 1', 'Category 2'], [lst1[0], lst2[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3366cb61-7fde-40c5-8ac3-61749ad21b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cats\n",
    "# [el[0] for el in vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7465e1-5ff6-4fce-a1f1-4da2f60507aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats, vals = zip(*Weights_variance.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa84f3-e104-42c9-9487-6b31992957e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals\n",
    "# cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e43864f-f5bd-4932-b8aa-f0f4c854a842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.animate import animated_bar_var_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4d3f4-632e-4ef3-ab75-6f181e3cb41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "animated_bar_var_plot(Weights_variance, filename = save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6175d6b-5a52-4f0e-9aac-6f9801c83529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "cats, vals = zip(*Weights_variance.items())\n",
    "fig, axes = plt.subplots(figsize=(8, 6))\n",
    "axes.set_ylim(0, 1)\n",
    "\n",
    "bars = axes.bar(cats, [el[0] for el in vals])\n",
    "cats, vals = zip(*Weights_variance.items())\n",
    "def animate(i):\n",
    "    for j in range(len(cats)):\n",
    "        bars[j].set_height(vals[j][i])\n",
    "\n",
    "plt.title(\"Intro to Animated Bar Plot\", fontsize=14)\n",
    "ani = FuncAnimation(fig, animate, frames=len(vals[0]), repeat=False)\n",
    "HTML(ani.to_jshtml())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7827508a-e72e-4ccf-b9e1-6b25d914523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Callable, Tuple, List\n",
    "def covert_dict2animate_format(weights_variance: dict) -> List[List]:\n",
    "    names = weights_variance.keys()\n",
    "    values = weights_variance.values()\n",
    "    return list(names), list(values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ca053c-e61f-4718-8779-6bd3cfd751e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert_dict2animate_format(Weights_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0113bc21-160d-4fcb-be62-389859781adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(8, 6))\n",
    "axes.set_ylim(0, 120)\n",
    "\n",
    "lst1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "lst2 = [0, 5, 10, 15, 20, 25, 30, 35, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "bars = axes.bar(['Category 1', 'Category 2'], [lst1[0], lst2[0]])\n",
    "\n",
    "def animate(i):\n",
    "    bars[0].set_height(lst1[i])\n",
    "    bars[1].set_height(lst2[i])\n",
    "\n",
    "plt.title(\"Intro to Animated Bar Plot\", fontsize=14)\n",
    "ani = FuncAnimation(fig, animate, frames=len(lst1), repeat=False)\n",
    "HTML(ani.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7f4aff-53cb-435d-ab81-7378a596c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1, q2 = weights_variances(G)\n",
    "q1, q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6431c-5b45-4f50-b6b9-60d246ca2bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1, r2 = weights_variances(G)\n",
    "r1, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5c2683-5a01-48a4-94b6-6c13b9880c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = {v: r1[k] for k, v in r2.items()}\n",
    "t2 = {v: q1[k] for k, v in q2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d7baf9-f012-4923-ad8b-7ac52298e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1, t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdbe998-e85d-47c0-91cd-5b4bf8dbf3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1.values(), q2.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5a93ae-61a5-4576-8e22-2aa0f7a8382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0607c113-75b9-49af-b1db-9c6c5a57ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(q2.values(), q1.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28547627-3fa5-4d8d-a2eb-1bc897ef564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea916ad-2f33-4eb8-bbda-64ab0d6f08ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db4437-71ab-4fa2-97e0-d6d11fcef970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install PyQt5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb510a-a617-49f5-93b0-cf144b268eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install PySide2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e7450-04c6-44c0-8ffe-fda13a54a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48dda9b-06ba-4a0e-940e-dd7de4065315",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_ylim(0, max(max(dic.values()) for dic in q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb1fba2-b199-4f1c-9716-32a91b3abe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import seaborn as sns\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6cb9d3-f922-4cec-a282-186e99b1fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "axes = fig.add_subplot(1,1,1)\n",
    "axes.set_ylim(0, 150)\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "lst1=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 ]\n",
    "lst2=[0, 5, 10, 15, 20, 25, 30, 35, 40, 50, 60, 70, 80, 90, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d828e8d-5ece-459f-b0eb-28010593a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "q = [t1, t2]\n",
    "i = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xdata, ydata = [], []\n",
    "ln, = ax.plot([], [], 'ro')\n",
    "\n",
    "def init():\n",
    "    ax.set_xlim(0, 2*np.pi)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    return ln,\n",
    "\n",
    "# def update(frame):\n",
    "    \n",
    "#     xdata.append(frame)\n",
    "#     ydata.append(np.sin(frame))\n",
    "#     ln.set_data(xdata, ydata)\n",
    "#     return ln,\n",
    "# x_labels = list(q[0].keys())\n",
    "def update(frame):\n",
    "    heights = [q[i][key] for key in x_labels]\n",
    "    for bar, height in zip(bars, heights):\n",
    "        bar.set_height(height)\n",
    "    return bars\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=np.linspace(0, 2*np.pi, 128),\n",
    "                    init_func=init, blit=True, repeat = False)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bff40e-0cf8-4009-b9fb-0a29dd227091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# Sample data for q\n",
    "q = [{'A': 3, 'B': 5, 'C': 2}, {'A': 1, 'B': 4, 'C': 6}, {'A': 2, 'B': 3, 'C': 5}]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xdata, ydata = [], []\n",
    "bars = ax.bar([], [])\n",
    "\n",
    "def init():\n",
    "    ax.set_ylim(0, max(max(dic.values()) for dic in q))\n",
    "    return bars\n",
    "\n",
    "def update(frame):\n",
    "    values = q[frame].values()\n",
    "    xdata.extend(range(len(values)))\n",
    "    ydata.extend(values)\n",
    "    \n",
    "    for bar, h in zip(bars, values):\n",
    "        bar.set_height(h)\n",
    "    \n",
    "    return bars\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=range(len(q)),\n",
    "                    init_func=init, blit=True, repeat=False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd6af7-77b8-4ac1-ada9-03f1e689f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xdata, ydata = [], []\n",
    "ln, = ax.plot([], [], 'ro')\n",
    "\n",
    "def init():\n",
    "    ax.set_xlim(0, 2*np.pi)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    return ln,\n",
    "\n",
    "def update(frame):\n",
    "    print(frame, np.sin(frame))\n",
    "    xdata.append(frame)\n",
    "    ydata.append(np.sin(frame))\n",
    "    ln.set_data(xdata, ydata)\n",
    "    return ln,\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=np.linspace(0, 2*np.pi, 128),\n",
    "                    init_func=init, blit=True, repeat = False)\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caedd287-3569-42b3-9b32-7f9fbefd66aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc891f-704e-4abb-bfd8-2d43ad45bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_losses_final = []\n",
    "# G_losses_final = []\n",
    "# Variances = []\n",
    "# # NUM_EPOCHS = NUM_EPOCHS//10\n",
    "# for epoch in tqdm(range(1, NUM_EPOCHS+1)):           \n",
    "#     D_losses, G_losses = [], []\n",
    "#     for batch_idx in range(batches_per_epoch):\n",
    "#         batch, mean_var = create_batch(\n",
    "#                                 Generator = G,\n",
    "#                                 batch_size = BATCH_SIZE,\n",
    "#                                 get_variance = True,\n",
    "#                                 repeat = 10,\n",
    "#                                 num_samples = 128\n",
    "#                             )\n",
    "#         x, info = batch#x is y coordiante, we predict it, info is x corrdinate, its condition\n",
    "#         D_losses.append(D_train(x, info))\n",
    "#         G_losses.append(G_train(x))\n",
    "#         Variances.append(mean_var)\n",
    "#         # print(len(batch), var, x.size(), info.size())\n",
    "#         # print(var)\n",
    "#         # break\n",
    "    \n",
    "#     # for batch_idx, (x, info) in enumerate(train_loader):\n",
    "#     #     D_losses.append(D_train(x, info))\n",
    "#     #     G_losses.append(G_train(x))\n",
    "        \n",
    "#     D_losses_final.append(torch.mean(torch.FloatTensor(D_losses)))\n",
    "#     G_losses_final.append(torch.mean(torch.FloatTensor(G_losses)))\n",
    "    \n",
    "#     if epoch % 20 == 0:\n",
    "#         print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % (\n",
    "#                 (epoch),\n",
    "#                 NUM_EPOCHS,\n",
    "#                 torch.mean(torch.FloatTensor(D_losses)),\n",
    "#                 torch.mean(torch.FloatTensor(G_losses))\n",
    "#                 )\n",
    "#              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b3a4c-cad0-4018-a926-83671ce4b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1, q2 = weights_variances(G)\n",
    "\n",
    "# q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14e53e1-8bc8-4a3e-a4a8-b72ab0a32f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_variancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd7e523-90e4-494f-8298-de46dfc7fe72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b1c6d-52a4-43b3-9120-e7e5370df275",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d581e033-1e17-4745-b66d-6708fd2f6870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68f725e-a195-4170-be30-11fb4e6a6d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q\n",
    "a = torch.zeros(32, 1)\n",
    "a.squeeze(-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c5ee5-6c85-41bc-8880-f906115489cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # calculate binary cross-entropy loss\n",
    "    loss = - (y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "\n",
    "    # apply weights if provided\n",
    "    if weights is not None:\n",
    "        loss = loss * weights\n",
    "\n",
    "    # take the mean across the batch\n",
    "    loss = torch.mean(loss)\n",
    "    \n",
    "    loss = F.binary_cross_entropy_with_logits(y_pred, y_true, weight=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e7db4-451c-43fb-ab5e-69792c5e4054",
   "metadata": {},
   "source": [
    "The structure of having an outer function (weighted_interval_crossentropy) that returns an inner function (loss) is a way to create a closure, allowing the inner function to access variables from the outer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a458b8-00d0-4aad-b119-cbd76073ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def weighted_interval_crossentropy():\n",
    "    \"\"\"\n",
    "    A weighted version of interval crossentropy loss for PyTorch.\n",
    "\n",
    "    weights: torch tensor of shape batch_size\n",
    "             torch.tensor([0.1, 0.3, 0.2,...]) # point one is in interval where the weight is 0.1, point 2 in interval the weight is 0.3, ...\n",
    "    \"\"\"\n",
    "    # weights = torch.tensor(weights, requires_grad=False)\n",
    "    \n",
    "\n",
    "    def loss(y_pred, y_true, weights, conditional_info, from_logits=False):\n",
    "        \"\"\"\n",
    "        y_true size: torch tensor (32, 1)\n",
    "        y_pred size: torch tensor (32, 1)\n",
    "        \"\"\"\n",
    "        weights = weights4batch(weights[0],\n",
    "                                conditional_info = conditional_info,\n",
    "                                bins_values = weights[1])# weights size: torch tensor (32)\n",
    "        if from_logits:\n",
    "            y_pred = F.softmax(y_pred, dim=-1)\n",
    "\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = torch.clamp(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        loss = F.binary_cross_entropy(y_pred, y_true, weight=weights.unsqueeze(-1))\n",
    "\n",
    "        # # calculate loss\n",
    "        # weights = weights.unsqueeze(-1)\n",
    "        # print(f'''weights: {weights}, \n",
    "        #       y_true: {y_true},\n",
    "        #       y_pred: {y_pred},\n",
    "        #       torch.log(y_pred): {torch.log(y_pred)}''')\n",
    "        # loss = y_true * torch.log(y_pred) * weights * 10\n",
    "        # loss = -torch.sum(loss.squeeze(-1), -1)\n",
    "        # print(f'loss: {loss}')\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# weights = [0.5, 2, 10]\n",
    "# loss_function = weighted_categorical_crossentropy(weights)\n",
    "# loss_value = loss_function(y_true, y_pred, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c2435-d976-4065-9301-7226c95a3b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6a3a4-0cf5-4e27-854f-3621721469e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.clamp(torch.tensor([0, 0.5, 1, 2, 3]), 1e-7, 1 - 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f57978-632f-4d78-b7f2-f3f3d451cab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6997605a-03c9-4fbb-9764-68e214e07421",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([1, 2, 3]), torch.log(torch.tensor([0, 0.5, 1, 2, 3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2c2a95-98d9-40b3-8bf4-76ada362d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = [0.5, 2, 10]\n",
    "loss_function = weighted_categorical_crossentropy(weights)\n",
    "loss_value = loss_function(y_true, y_pred, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8489d72f-4a9a-4b99-a7ea-c529e08df103",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5893b-2b07-45e5-a56d-7fd3c62ba961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb9a743-8406-44ec-80e1-503a55631bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e2f96b-74af-47ff-8950-6b03805a13e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_interval_crossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83239648-ef5d-4777-9027-05ced459a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_interval_crossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb34561-9292-4a1f-8385-d52a4867f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d11af3f-67ea-43b8-a674-1fe51d6d3906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights4batch(weights:dict, conditional_info, bins_values:dict):\n",
    "    \n",
    "    \"\"\"\n",
    "    Assign weights for every point based on conditional information and bins.\n",
    "\n",
    "    Args:\n",
    "    - weights (dict): Weights for every interval.\n",
    "    - conditional_info (torch.Tensor): Tensor with conditional information (coordinate x of sine).\n",
    "    - bins_values (dict): Bins values.\n",
    "\n",
    "    Returns:\n",
    "    - torch.tensor: Weights for every point that we have in condition.\n",
    "    \"\"\"\n",
    "    weights_bins = []\n",
    "    number_bins = len(bins_values.values())\n",
    "    # print(f'conditional_info: {conditional_info}')\n",
    "    sorted_info, indices = torch.sort(conditional_info, axis = 0)\n",
    "\n",
    "    \n",
    "    current_bin_id = 1\n",
    "    # print(f'sorted_info and indices: {sorted_info}, {indices}')\n",
    "    for i in range(conditional_info.size(0)):\n",
    "        #find the next current_bin_id\n",
    "        while current_bin_id <= number_bins - 1 and not (\n",
    "            bins_values[current_bin_id] < sorted_info[i].item() < bins_values[current_bin_id + 1]\n",
    "        ):\n",
    "            current_bin_id += 1\n",
    "            \n",
    "        weights_bins.append(weights[current_bin_id])\n",
    "        \n",
    "    weights_bins = torch.tensor(weights_bins)\n",
    "    sorted_weights = weights_bins[torch.argsort(indices.view(-1))]\n",
    "    # print(torch.argsort(indices.view(-1)))\n",
    "    return sorted_weights\n",
    "\n",
    "\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93910aef-ee42-432f-8152-2fa46f5e56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_info_0 = 2 * math.pi * torch.rand(4).view(-1, 1).to(device)\n",
    "# conditional_info_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7e5669-faa4-45a0-96e5-ea58021002df",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1, q2\n",
    "# conditional_info_0\n",
    "# q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c62536-a415-4e83-b40f-0efbfe754d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec002369-10af-47c4-82db-091b6cce76e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_info_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dbbdde-edd3-4e45-bc4d-a57974cc2401",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = weights4batch(q1, conditional_info_0, q2)\n",
    "# e[1]\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0f0ce-8b2c-4d6b-a77a-8d6f7fc7bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb6b0e-c994-4052-a8a2-674ec6208ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e[1][e[0]]\n",
    "# e[1]\n",
    "# [e[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80705b95-0447-4b3e-8f7b-0d3f6baba589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices\n",
    "# bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd9d0dd-7f40-4f2d-bf94-56def118960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.log(torch.tensor([torch.tensor(i) for i in range(1, 33)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f4964-0627-4b94-9e09-6490e432e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted, indices = torch.sort(fi, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16709e09-c7e1-44e5-aa1f-5e3fe81614ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.size(), r.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f75c87-1d2c-4d4b-95a8-22f2a511e8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r\n",
    "e.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b004d15e-2c91-4510-8771-9b6cdf572f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2a3ff2-9e99-4f66-abbd-9263cd4fb259",
   "metadata": {},
   "outputs": [],
   "source": [
    "r * e\n",
    "# y_true * torch.log(y_pred) * weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1338c7-a88c-453e-8e03-086e5c70fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = 2 * math.pi * torch.rand(32).view(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e35020-f564-4037-8ab5-abdbf4ce1ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f216cf6b-003c-4e93-bf53-ea3d379723c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1283990-6c37-4d61-8dc4-e46fe78d66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sine(G, save_path = '/Users/serafim/Desktop/Job/projects/science/hse/GAN-Estimation-Uncertainty/uncertainty/tests/', name = 'base_gan__base_training__1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d975aa28-ea85-4bfe-a8df-cdfabc4f1595",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1603945-14c0-43da-9ce0-1326ea8c85f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ad05b18-c5fd-421d-8e9e-520f45cac171",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e7a69d-8e57-4852-ae05-ba950601fcc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e03cbf-9fe5-421c-8761-1728d0fac208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_space_samples = torch.randn(1000, 2)\n",
    "num_samples = 10000\n",
    "latent_space_samples = torch.randn(num_samples, 1)\n",
    "# info = 8 * math.pi * torch.rand(num_samples).view(-1, 1) - 4 * math.pi\n",
    "info = 2 * math.pi * torch.rand(num_samples).view(-1, 1)\n",
    "generated_samples = G(latent_space_samples, info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b701fa7-43e7-4a80-991f-b7b80f6aa276",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_x, res = calculate_variance(G, repeat = 10, num_samples = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b55449a-f2c8-4c45-92be-c4f5ead9f99e",
   "metadata": {},
   "source": [
    "##### 4\n",
    "> num_samples = 128, lr = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58eec43-8832-4302-9216-3d1a12cde4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples = generated_samples.detach()\n",
    "plt.plot(info, generated_samples, 'ko', markersize = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da9784e-b9e7-4df4-9c19-76818bcb7de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "plt.plot(points_x, res, 'ko', markersize = 3, label='Variancies')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Model variance')\n",
    "plt.title('Model variancies at Different Points')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75103cda-aa06-4fdf-bdc4-7d305bac861a",
   "metadata": {},
   "source": [
    "##### 3\n",
    "> num_samples = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5950f-205c-435a-8154-790209cdce54",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples = generated_samples.detach()\n",
    "plt.plot(info, generated_samples, 'ko', markersize = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3132ddb8-b1dc-4842-ad3c-034d0d668afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "plt.plot(points_x, res, 'ko', markersize = 3, label='Variancies')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Model variance')\n",
    "plt.title('Model variancies at Different Points')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467c0887-d72a-4590-a20b-459c289e751f",
   "metadata": {},
   "source": [
    "##### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b69d4f9-1b35-40d0-a973-7ebad7172865",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples = generated_samples.detach()\n",
    "plt.plot(info, generated_samples, 'ko', markersize = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4491263d-90d4-4b2b-aeea-c417258766ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "plt.plot(points_x, res, 'ko', markersize = 3, label='Variancies')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Model variance')\n",
    "plt.title('Model variancies at Different Points')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52a507-4f31-4116-a891-ae073b42619c",
   "metadata": {},
   "source": [
    "##### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3778d721-24df-4970-be17-0f2bf2940d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples = generated_samples.detach()\n",
    "plt.plot(info, generated_samples, 'ko', markersize = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddedbcbc-7a5d-43a1-b2ef-8839d25f906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "plt.plot(points_x, res, 'ko', markersize = 3, label='Variancies')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Model variance')\n",
    "plt.title('Model variancies at Different Points')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a702eb0-6663-4022-bc56-c316acf5f54a",
   "metadata": {},
   "source": [
    "#### basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c9a71-452f-4aaa-a9bd-9f41cc117344",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples = generated_samples.detach()\n",
    "plt.plot(info, generated_samples, 'ko', markersize = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185c9d14-b26b-48ad-a240-35df60f6ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_x, res = calculate_variance(G, repeat = 1000, num_samples = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117ca9c6-7405-472d-a781-bc0bd7bc7f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "plt.plot(points_x, res, 'ko', markersize = 3, label='Variancies')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Model variance')\n",
    "plt.title('Model variancies at Different Points')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b2773-b81e-4796-8e9c-29fb4269ec10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258cea36-68c8-4b86-9881-d970a7371223",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_batch(G, batch_size = 10,                  \n",
    "                 get_variance = True,\n",
    "                 repeat = 10,\n",
    "                 num_samples = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b73b44-904c-4d35-8ec6-c9481677b726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
